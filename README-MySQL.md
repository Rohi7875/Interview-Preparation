# MySQL Interview Questions & Answers
### For 7+ Years Experienced PHP Developers

---

## Table of Contents
1. [MySQL Basics](#mysql-basics)
2. [Data Types](#data-types)
3. [Queries & Joins](#queries--joins)
4. [Indexes & Performance](#indexes--performance)
5. [Transactions & Locking](#transactions--locking)
6. [Stored Procedures & Functions](#stored-procedures--functions)
7. [Triggers & Events](#triggers--events)
8. [Database Design](#database-design)
9. [Security](#security)
10. [Optimization & Best Practices](#optimization--best-practices)
11. [Advanced MySQL Concepts](#advanced-mysql-concepts)
12. [MySQL Replication](#mysql-replication)
13. [MySQL Partitioning](#mysql-partitioning)
14. [MySQL Backup & Recovery](#mysql-backup--recovery)
15. [MySQL Monitoring & Logging](#mysql-monitoring--logging)
16. [MySQL Performance Tuning](#mysql-performance-tuning)
17. [MySQL Security Hardening](#mysql-security-hardening)
18. [MySQL JSON Functions](#mysql-json-functions)
19. [MySQL Window Functions](#mysql-window-functions)
20. [MySQL Common Table Expressions](#mysql-common-table-expressions)

---

## MySQL Basics

### Q1. What are the differences between MySQL storage engines (InnoDB vs MyISAM)?
**Answer:**

| Feature | InnoDB | MyISAM |
|---------|--------|---------|
| Transactions | Supported | Not supported |
| Foreign Keys | Supported | Not supported |
| Row-level locking | Yes | Table-level locking |
| ACID compliance | Yes | No |
| Crash recovery | Automatic | Manual |
| Full-text search | Supported (MySQL 5.6+) | Supported |
| Performance | Better for writes | Better for reads |
| Default engine | Yes (MySQL 5.5+) | No |

**Real-time Example:**
```sql
-- Creating table with InnoDB (default)
CREATE TABLE users (
    id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    username VARCHAR(50) NOT NULL UNIQUE,
    email VARCHAR(100) NOT NULL UNIQUE,
    password VARCHAR(255) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    INDEX idx_username (username),
    INDEX idx_email (email)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- Creating table with MyISAM (for read-heavy operations)
CREATE TABLE logs (
    id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    user_id INT UNSIGNED,
    action VARCHAR(100),
    ip_address VARCHAR(45),
    user_agent TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_user_id (user_id),
    INDEX idx_action (action),
    INDEX idx_created_at (created_at),
    FULLTEXT KEY ft_user_agent (user_agent)
) ENGINE=MyISAM DEFAULT CHARSET=utf8mb4;

-- Check table engine
SELECT 
    TABLE_NAME,
    ENGINE,
    TABLE_ROWS,
    AVG_ROW_LENGTH,
    DATA_LENGTH,
    INDEX_LENGTH
FROM information_schema.TABLES
WHERE TABLE_SCHEMA = 'mydb';

-- Convert table engine
ALTER TABLE logs ENGINE=InnoDB;
```

### Q2. What is normalization and its types?
**Answer:** Normalization is the process of organizing database tables to reduce redundancy and dependency.

**Normalization Forms:**
1. **1NF (First Normal Form)** - Eliminate repeating groups, atomic values
2. **2NF (Second Normal Form)** - Meet 1NF + No partial dependencies
3. **3NF (Third Normal Form)** - Meet 2NF + No transitive dependencies
4. **BCNF (Boyce-Codd Normal Form)** - Stronger version of 3NF
5. **4NF** - Meet BCNF + No multi-valued dependencies
6. **5NF** - Meet 4NF + No join dependencies

### Q2a. Complete Normalization Process (1NF, 2NF, 3NF, BCNF)
**Answer:** Step-by-step normalization process to eliminate data redundancy and anomalies.

**Real-time Example:**
```sql
-- STEP 1: UNNORMALIZED TABLE (Original messy data)
CREATE TABLE student_courses_unnormalized (
    student_id INT,
    student_name VARCHAR(100),
    student_address VARCHAR(200),
    course_id INT,
    course_name VARCHAR(100),
    instructor_name VARCHAR(100),
    instructor_office VARCHAR(50),
    grade CHAR(2),
    credits INT,
    semester VARCHAR(20)
);

INSERT INTO student_courses_unnormalized VALUES
(1, 'John Doe', '123 Main St', 101, 'Database Design', 'Dr. Smith', 'Room 201', 'A', 3, 'Fall 2023'),
(1, 'John Doe', '123 Main St', 102, 'Web Development', 'Dr. Johnson', 'Room 205', 'B+', 4, 'Fall 2023'),
(2, 'Jane Smith', '456 Oak Ave', 101, 'Database Design', 'Dr. Smith', 'Room 201', 'A-', 3, 'Fall 2023'),
(2, 'Jane Smith', '456 Oak Ave', 103, 'Data Structures', 'Dr. Brown', 'Room 203', 'B', 3, 'Spring 2024');

-- PROBLEMS:
-- 1. Student data repeated for each course
-- 2. Course data repeated for each student
-- 3. Instructor data repeated
-- 4. Update anomalies everywhere!

-- STEP 2: FIRST NORMAL FORM (1NF) - Eliminate repeating groups
-- Each cell contains atomic values, no repeating groups

CREATE TABLE students_1nf (
    student_id INT PRIMARY KEY,
    student_name VARCHAR(100),
    student_address VARCHAR(200)
);

CREATE TABLE courses_1nf (
    course_id INT PRIMARY KEY,
    course_name VARCHAR(100),
    instructor_name VARCHAR(100),
    instructor_office VARCHAR(50),
    credits INT
);

CREATE TABLE enrollments_1nf (
    student_id INT,
    course_id INT,
    grade CHAR(2),
    semester VARCHAR(20),
    PRIMARY KEY (student_id, course_id),
    FOREIGN KEY (student_id) REFERENCES students_1nf(student_id),
    FOREIGN KEY (course_id) REFERENCES courses_1nf(course_id)
);

-- STEP 3: SECOND NORMAL FORM (2NF) - Remove partial dependencies
-- All non-key attributes must depend on the ENTIRE primary key

-- Current enrollments_1nf is already in 2NF because:
-- - grade depends on (student_id, course_id) combination
-- - semester depends on (student_id, course_id) combination

-- But let's check courses_1nf:
-- instructor_name and instructor_office depend only on course_id, not on the full key
-- This is OK because course_id IS the primary key

-- However, if we had a composite key, we'd need to separate:

CREATE TABLE courses_2nf (
    course_id INT PRIMARY KEY,
    course_name VARCHAR(100),
    credits INT
);

CREATE TABLE instructors_2nf (
    instructor_id INT PRIMARY KEY,
    instructor_name VARCHAR(100),
    instructor_office VARCHAR(50)
);

CREATE TABLE course_instructors_2nf (
    course_id INT,
    instructor_id INT,
    PRIMARY KEY (course_id, instructor_id),
    FOREIGN KEY (course_id) REFERENCES courses_2nf(course_id),
    FOREIGN KEY (instructor_id) REFERENCES instructors_2nf(instructor_id)
);

-- STEP 4: THIRD NORMAL FORM (3NF) - Remove transitive dependencies
-- No non-key attribute should depend on another non-key attribute

-- Let's add more attributes to demonstrate 3NF:
CREATE TABLE students_3nf_demo (
    student_id INT PRIMARY KEY,
    student_name VARCHAR(100),
    student_address VARCHAR(200),
    city VARCHAR(50),
    state VARCHAR(50),
    zip_code VARCHAR(10),
    country VARCHAR(50)
);

-- PROBLEM: city, state, zip_code depend on student_address, not directly on student_id
-- This creates transitive dependency: student_id â†’ student_address â†’ city/state/zip

-- SOLUTION: Separate address components
CREATE TABLE students_3nf (
    student_id INT PRIMARY KEY,
    student_name VARCHAR(100),
    address_id INT,
    FOREIGN KEY (address_id) REFERENCES addresses(address_id)
);

CREATE TABLE addresses (
    address_id INT PRIMARY KEY,
    street_address VARCHAR(200),
    city VARCHAR(50),
    state VARCHAR(50),
    zip_code VARCHAR(10),
    country VARCHAR(50)
);

-- STEP 5: BOYCE-CODD NORMAL FORM (BCNF) - Every determinant is a candidate key
-- More strict than 3NF

-- Example where 3NF is satisfied but BCNF is not:
CREATE TABLE course_schedule_demo (
    course_id INT,
    instructor_id INT,
    semester VARCHAR(20),
    classroom VARCHAR(50),
    PRIMARY KEY (course_id, semester)
);

-- Assume: Each instructor teaches only one course per semester
-- This means: instructor_id â†’ course_id (instructor determines course)
-- But instructor_id is not a candidate key!

-- BCNF SOLUTION:
CREATE TABLE instructor_courses (
    instructor_id INT PRIMARY KEY,
    course_id INT,
    FOREIGN KEY (course_id) REFERENCES courses(course_id)
);

CREATE TABLE course_schedule_bcnf (
    course_id INT,
    semester VARCHAR(20),
    classroom VARCHAR(50),
    PRIMARY KEY (course_id, semester),
    FOREIGN KEY (course_id) REFERENCES instructor_courses(course_id)
);

-- COMPLETE NORMALIZED DATABASE:
CREATE TABLE students (
    student_id INT PRIMARY KEY AUTO_INCREMENT,
    student_name VARCHAR(100) NOT NULL,
    email VARCHAR(100) UNIQUE,
    phone VARCHAR(20),
    address_id INT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (address_id) REFERENCES addresses(address_id)
);

CREATE TABLE addresses (
    address_id INT PRIMARY KEY AUTO_INCREMENT,
    street_address VARCHAR(200),
    city VARCHAR(50),
    state VARCHAR(50),
    zip_code VARCHAR(10),
    country VARCHAR(50) DEFAULT 'USA'
);

CREATE TABLE departments (
    dept_id INT PRIMARY KEY AUTO_INCREMENT,
    dept_name VARCHAR(100) NOT NULL,
    dept_head VARCHAR(100)
);

CREATE TABLE instructors (
    instructor_id INT PRIMARY KEY AUTO_INCREMENT,
    instructor_name VARCHAR(100) NOT NULL,
    email VARCHAR(100) UNIQUE,
    office_location VARCHAR(50),
    dept_id INT,
    FOREIGN KEY (dept_id) REFERENCES departments(dept_id)
);

CREATE TABLE courses (
    course_id INT PRIMARY KEY AUTO_INCREMENT,
    course_code VARCHAR(10) UNIQUE,
    course_name VARCHAR(100) NOT NULL,
    credits INT NOT NULL,
    dept_id INT,
    FOREIGN KEY (dept_id) REFERENCES departments(dept_id)
);

CREATE TABLE course_instructors (
    course_id INT,
    instructor_id INT,
    PRIMARY KEY (course_id, instructor_id),
    FOREIGN KEY (course_id) REFERENCES courses(course_id),
    FOREIGN KEY (instructor_id) REFERENCES instructors(instructor_id)
);

CREATE TABLE semesters (
    semester_id INT PRIMARY KEY AUTO_INCREMENT,
    semester_name VARCHAR(20) NOT NULL,
    start_date DATE,
    end_date DATE
);

CREATE TABLE enrollments (
    enrollment_id INT PRIMARY KEY AUTO_INCREMENT,
    student_id INT,
    course_id INT,
    semester_id INT,
    grade CHAR(2),
    enrollment_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE KEY unique_enrollment (student_id, course_id, semester_id),
    FOREIGN KEY (student_id) REFERENCES students(student_id),
    FOREIGN KEY (course_id) REFERENCES courses(course_id),
    FOREIGN KEY (semester_id) REFERENCES semesters(semester_id)
);

-- Insert sample data
INSERT INTO addresses VALUES
(1, '123 Main St', 'New York', 'NY', '10001', 'USA'),
(2, '456 Oak Ave', 'Boston', 'MA', '02101', 'USA');

INSERT INTO departments VALUES
(1, 'Computer Science', 'Dr. Wilson'),
(2, 'Mathematics', 'Dr. Davis');

INSERT INTO students VALUES
(1, 'John Doe', 'john@email.com', '555-0101', 1, NOW()),
(2, 'Jane Smith', 'jane@email.com', '555-0102', 2, NOW());

INSERT INTO instructors VALUES
(1, 'Dr. Smith', 'smith@university.edu', 'Room 201', 1),
(2, 'Dr. Johnson', 'johnson@university.edu', 'Room 205', 1);

INSERT INTO courses VALUES
(1, 'CS101', 'Database Design', 3, 1),
(2, 'CS102', 'Web Development', 4, 1),
(3, 'MATH201', 'Calculus', 4, 2);

INSERT INTO course_instructors VALUES
(1, 1), (2, 2);

INSERT INTO semesters VALUES
(1, 'Fall 2023', '2023-09-01', '2023-12-15'),
(2, 'Spring 2024', '2024-01-15', '2024-05-15');

INSERT INTO enrollments VALUES
(1, 1, 1, 1, 'A', NOW()),
(2, 1, 2, 1, 'B+', NOW()),
(3, 2, 1, 1, 'A-', NOW()),
(4, 2, 3, 2, 'B', NOW());

-- BENEFITS OF NORMALIZATION:
-- 1. Eliminates data redundancy
-- 2. Prevents update anomalies
-- 3. Prevents insert anomalies
-- 4. Prevents delete anomalies
-- 5. Ensures data consistency
-- 6. Saves storage space
-- 7. Makes queries more efficient
-- 8. Easier to maintain and modify

-- QUERY EXAMPLES:
-- Get all courses taken by John Doe
SELECT c.course_name, e.grade, s.semester_name
FROM students st
JOIN enrollments e ON st.student_id = e.student_id
JOIN courses c ON e.course_id = c.course_id
JOIN semesters s ON e.semester_id = s.semester_id
WHERE st.student_name = 'John Doe';

-- Get instructor for each course
SELECT c.course_name, i.instructor_name, i.office_location
FROM courses c
JOIN course_instructors ci ON c.course_id = ci.course_id
JOIN instructors i ON ci.instructor_id = i.instructor_id;
```

### Q2b. Denormalization - When and Why?
**Answer:** Denormalization is the process of adding redundant data to improve query performance, often at the cost of storage space and update complexity.

**Real-time Example:**
```sql
-- SCENARIO: E-commerce system with high read traffic
-- Problem: Normalized tables require multiple JOINs for common queries

-- NORMALIZED APPROACH (Good for consistency, bad for performance)
CREATE TABLE products_normalized (
    product_id INT PRIMARY KEY,
    product_name VARCHAR(100),
    category_id INT,
    brand_id INT,
    price DECIMAL(10,2),
    FOREIGN KEY (category_id) REFERENCES categories(category_id),
    FOREIGN KEY (brand_id) REFERENCES brands(brand_id)
);

CREATE TABLE categories (
    category_id INT PRIMARY KEY,
    category_name VARCHAR(50),
    parent_category_id INT
);

CREATE TABLE brands (
    brand_id INT PRIMARY KEY,
    brand_name VARCHAR(50),
    country VARCHAR(50)
);

-- Query to get product details (requires JOINs)
SELECT 
    p.product_name,
    c.category_name,
    b.brand_name,
    p.price
FROM products_normalized p
JOIN categories c ON p.category_id = c.category_id
JOIN brands b ON p.brand_id = b.brand_id
WHERE p.product_id = 123;

-- DENORMALIZED APPROACH (Good for performance, requires careful updates)
CREATE TABLE products_denormalized (
    product_id INT PRIMARY KEY,
    product_name VARCHAR(100),
    category_name VARCHAR(50),        -- Denormalized
    brand_name VARCHAR(50),           -- Denormalized
    brand_country VARCHAR(50),        -- Denormalized
    price DECIMAL(10,2),
    stock_quantity INT,
    avg_rating DECIMAL(3,2),         -- Denormalized (calculated field)
    total_reviews INT,               -- Denormalized (calculated field)
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
);

-- Query to get product details (no JOINs needed!)
SELECT 
    product_name,
    category_name,
    brand_name,
    price,
    avg_rating,
    total_reviews
FROM products_denormalized
WHERE product_id = 123;

-- WHEN TO DENORMALIZE:

-- 1. READ-HEAVY APPLICATIONS
-- Example: Product catalog that's read 1000x more than updated
CREATE TABLE product_summary (
    product_id INT PRIMARY KEY,
    product_name VARCHAR(100),
    category_name VARCHAR(50),
    brand_name VARCHAR(50),
    price DECIMAL(10,2),
    discount_price DECIMAL(10,2),
    final_price DECIMAL(10,2),      -- Calculated field
    in_stock BOOLEAN,               -- Calculated field
    popularity_score INT,           -- Calculated field
    last_calculated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 2. REPORTING TABLES
-- Example: Pre-calculated analytics
CREATE TABLE daily_sales_summary (
    summary_date DATE PRIMARY KEY,
    total_orders INT,
    total_revenue DECIMAL(12,2),
    total_customers INT,
    avg_order_value DECIMAL(10,2),
    top_selling_product VARCHAR(100),
    top_selling_category VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 3. CACHING FREQUENTLY ACCESSED DATA
-- Example: User dashboard data
CREATE TABLE user_dashboard_cache (
    user_id INT PRIMARY KEY,
    total_orders INT,
    total_spent DECIMAL(12,2),
    favorite_category VARCHAR(50),
    last_order_date DATE,
    loyalty_points INT,
    membership_level VARCHAR(20),
    cache_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
);

-- 4. MATERIALIZED VIEWS (MySQL 8.0+)
-- Example: Complex aggregations
CREATE TABLE product_analytics_mv (
    product_id INT PRIMARY KEY,
    total_sales INT,
    total_revenue DECIMAL(12,2),
    avg_rating DECIMAL(3,2),
    review_count INT,
    return_rate DECIMAL(5,2),
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
);

-- DENORMALIZATION STRATEGIES:

-- 1. CALCULATED FIELDS
-- Update triggers to maintain calculated fields
DELIMITER //
CREATE TRIGGER update_product_stats
AFTER INSERT ON order_items
FOR EACH ROW
BEGIN
    UPDATE products_denormalized p
    SET 
        total_sold = (
            SELECT COALESCE(SUM(quantity), 0)
            FROM order_items oi
            JOIN orders o ON oi.order_id = o.order_id
            WHERE oi.product_id = NEW.product_id AND o.status = 'delivered'
        ),
        last_updated = NOW()
    WHERE p.product_id = NEW.product_id;
END //
DELIMITER ;

-- 2. BATCH UPDATES FOR DENORMALIZED DATA
-- Stored procedure to refresh denormalized tables
DELIMITER //
CREATE PROCEDURE RefreshProductAnalytics()
BEGIN
    DECLARE EXIT HANDLER FOR SQLEXCEPTION
    BEGIN
        ROLLBACK;
        RESIGNAL;
    END;
    
    START TRANSACTION;
    
    -- Update product analytics
    UPDATE product_analytics_mv pa
    JOIN (
        SELECT 
            oi.product_id,
            SUM(oi.quantity) as total_sales,
            SUM(oi.quantity * oi.price) as total_revenue,
            AVG(r.rating) as avg_rating,
            COUNT(r.review_id) as review_count
        FROM order_items oi
        LEFT JOIN reviews r ON oi.product_id = r.product_id
        GROUP BY oi.product_id
    ) stats ON pa.product_id = stats.product_id
    SET 
        pa.total_sales = stats.total_sales,
        pa.total_revenue = stats.total_revenue,
        pa.avg_rating = COALESCE(stats.avg_rating, 0),
        pa.review_count = stats.review_count,
        pa.last_updated = NOW();
    
    COMMIT;
END //
DELIMITER ;

-- 3. EVENT TO REFRESH DENORMALIZED DATA
CREATE EVENT refresh_denormalized_data
ON SCHEDULE EVERY 1 HOUR
DO
CALL RefreshProductAnalytics();

-- TRADE-OFFS OF DENORMALIZATION:

-- ADVANTAGES:
-- âœ“ Faster queries (no JOINs)
-- âœ“ Reduced server load
-- âœ“ Simpler application code
-- âœ“ Better performance for read-heavy workloads
-- âœ“ Easier to scale horizontally

-- DISADVANTAGES:
-- âœ— Increased storage requirements
-- âœ— Data redundancy
-- âœ— Update complexity
-- âœ— Potential data inconsistency
-- âœ— More complex maintenance
-- âœ— Harder to maintain data integrity

-- BEST PRACTICES FOR DENORMALIZATION:

-- 1. Identify hot queries first
-- 2. Denormalize only frequently accessed data
-- 3. Use triggers or events to maintain consistency
-- 4. Monitor data freshness
-- 5. Have rollback strategy
-- 6. Document denormalization decisions
-- 7. Consider using materialized views
-- 8. Implement proper indexing on denormalized tables

-- Example: Hybrid approach - Keep normalized for OLTP, denormalized for OLAP
CREATE TABLE orders_normalized (
    order_id INT PRIMARY KEY,
    customer_id INT,
    order_date TIMESTAMP,
    status VARCHAR(20),
    total_amount DECIMAL(10,2)
);

CREATE TABLE orders_denormalized_for_reporting (
    order_id INT PRIMARY KEY,
    customer_name VARCHAR(100),
    customer_email VARCHAR(100),
    order_date TIMESTAMP,
    status VARCHAR(20),
    total_amount DECIMAL(10,2),
    product_names TEXT,  -- Comma-separated list
    category_names TEXT, -- Comma-separated list
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
);
```

### Q2c. ACID Properties - Database Transaction Guarantees
**Answer:** ACID properties ensure reliable database transactions even in case of system failures.

**Real-time Example:**
```sql
-- ACID PROPERTIES DEMONSTRATION

-- 1. ATOMICITY - "All or Nothing"
-- Either all operations succeed or all fail

-- Example: Bank transfer transaction
DELIMITER //
CREATE PROCEDURE BankTransfer(
    IN from_account INT,
    IN to_account INT,
    IN amount DECIMAL(10,2)
)
BEGIN
    DECLARE EXIT HANDLER FOR SQLEXCEPTION
    BEGIN
        ROLLBACK;
        RESIGNAL;
    END;
    
    START TRANSACTION;
    
    -- Check sufficient balance
    IF (SELECT balance FROM accounts WHERE account_id = from_account) < amount THEN
        SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Insufficient balance';
    END IF;
    
    -- Debit from source account
    UPDATE accounts 
    SET balance = balance - amount 
    WHERE account_id = from_account;
    
    -- Credit to destination account
    UPDATE accounts 
    SET balance = balance + amount 
    WHERE account_id = to_account;
    
    -- Log transaction
    INSERT INTO transaction_log (from_account, to_account, amount, timestamp)
    VALUES (from_account, to_account, amount, NOW());
    
    COMMIT;
    
    -- ATOMICITY: If ANY step fails, ALL changes are rolled back
END //
DELIMITER ;

-- Test atomicity
CALL BankTransfer(1, 2, 100.00);
-- Either all 3 operations succeed, or none do

-- 2. CONSISTENCY - "Valid State Always"
-- Database remains in a valid state before and after transaction

-- Example: Inventory management
CREATE TABLE products (
    product_id INT PRIMARY KEY,
    product_name VARCHAR(100),
    stock_quantity INT NOT NULL CHECK (stock_quantity >= 0),
    reserved_quantity INT NOT NULL CHECK (reserved_quantity >= 0),
    available_quantity INT GENERATED ALWAYS AS (stock_quantity - reserved_quantity) STORED
);

DELIMITER //
CREATE PROCEDURE ReserveInventory(
    IN p_product_id INT,
    IN p_quantity INT
)
BEGIN
    DECLARE current_stock INT;
    DECLARE EXIT HANDLER FOR SQLEXCEPTION
    BEGIN
        ROLLBACK;
        RESIGNAL;
    END;
    
    START TRANSACTION;
    
    -- Get current stock
    SELECT stock_quantity INTO current_stock
    FROM products WHERE product_id = p_product_id FOR UPDATE;
    
    -- Check availability
    IF current_stock < p_quantity THEN
        SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Insufficient stock';
    END IF;
    
    -- Reserve inventory
    UPDATE products 
    SET reserved_quantity = reserved_quantity + p_quantity
    WHERE product_id = p_product_id;
    
    COMMIT;
    
    -- CONSISTENCY: Stock constraints are always maintained
END //
DELIMITER ;

-- 3. ISOLATION - "Concurrent Transactions Don't Interfere"
-- Transactions execute independently without interference

-- Example: Concurrent order processing
CREATE TABLE orders (
    order_id INT PRIMARY KEY AUTO_INCREMENT,
    customer_id INT,
    total_amount DECIMAL(10,2),
    status VARCHAR(20) DEFAULT 'pending',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Session 1: Processing order
START TRANSACTION;
SELECT * FROM orders WHERE order_id = 1 FOR UPDATE;
-- Process payment, inventory, etc.
UPDATE orders SET status = 'processing' WHERE order_id = 1;
-- Don't commit yet

-- Session 2: Trying to modify same order
START TRANSACTION;
SELECT * FROM orders WHERE order_id = 1 FOR UPDATE;
-- This will wait until Session 1 commits or rolls back
-- ISOLATION: Prevents concurrent modifications

-- 4. DURABILITY - "Committed Changes Persist"
-- Once committed, changes survive system failures

-- MySQL ensures durability through:
-- 1. Write-ahead logging (WAL)
-- 2. Transaction logs
-- 3. Redo logs
-- 4. Binary logs

-- Configuration for durability:
SET GLOBAL innodb_flush_log_at_trx_commit = 1;  -- Most durable
SET GLOBAL sync_binlog = 1;                     -- Sync binary log
SET GLOBAL innodb_doublewrite = ON;             -- Double-write buffer

-- ACID PROPERTIES IN PRACTICE:

-- Example: E-commerce order processing
DELIMITER //
CREATE PROCEDURE ProcessOrder(
    IN p_customer_id INT,
    IN p_product_id INT,
    IN p_quantity INT
)
BEGIN
    DECLARE product_price DECIMAL(10,2);
    DECLARE total_amount DECIMAL(10,2);
    DECLARE order_id INT;
    
    DECLARE EXIT HANDLER FOR SQLEXCEPTION
    BEGIN
        ROLLBACK;
        RESIGNAL;
    END;
    
    START TRANSACTION;
    
    -- ATOMICITY: All operations in one transaction
    
    -- Get product price
    SELECT price INTO product_price
    FROM products WHERE product_id = p_product_id FOR UPDATE;
    
    -- Check stock availability
    IF (SELECT stock_quantity FROM products WHERE product_id = p_product_id) < p_quantity THEN
        SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Insufficient stock';
    END IF;
    
    -- Create order
    SET total_amount = product_price * p_quantity;
    INSERT INTO orders (customer_id, total_amount, status)
    VALUES (p_customer_id, total_amount, 'pending');
    SET order_id = LAST_INSERT_ID();
    
    -- Add order items
    INSERT INTO order_items (order_id, product_id, quantity, price)
    VALUES (order_id, p_product_id, p_quantity, product_price);
    
    -- Update inventory
    UPDATE products 
    SET stock_quantity = stock_quantity - p_quantity
    WHERE product_id = p_product_id;
    
    -- Update customer statistics
    UPDATE customers 
    SET total_orders = total_orders + 1,
        total_spent = total_spent + total_amount
    WHERE customer_id = p_customer_id;
    
    COMMIT;
    
    -- CONSISTENCY: All constraints maintained
    -- ISOLATION: No interference from concurrent transactions
    -- DURABILITY: Changes persisted to disk
    
    SELECT order_id as new_order_id;
END //
DELIMITER ;

-- ACID VIOLATIONS AND SOLUTIONS:

-- 1. ATOMICITY VIOLATION
-- Problem: Partial updates
UPDATE accounts SET balance = balance - 100 WHERE id = 1; -- Succeeds
-- System crashes here
UPDATE accounts SET balance = balance + 100 WHERE id = 2; -- Never executes

-- Solution: Use transactions
START TRANSACTION;
UPDATE accounts SET balance = balance - 100 WHERE id = 1;
UPDATE accounts SET balance = balance + 100 WHERE id = 2;
COMMIT;

-- 2. CONSISTENCY VIOLATION
-- Problem: Invalid data states
UPDATE products SET stock_quantity = -5 WHERE product_id = 1; -- Violates constraint

-- Solution: Use constraints and validation
ALTER TABLE products ADD CONSTRAINT chk_stock CHECK (stock_quantity >= 0);

-- 3. ISOLATION VIOLATION
-- Problem: Dirty reads, non-repeatable reads, phantom reads

-- Solution: Use appropriate isolation levels
SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;

-- 4. DURABILITY VIOLATION
-- Problem: Committed data lost after system failure

-- Solution: Proper configuration
SET GLOBAL innodb_flush_log_at_trx_commit = 1;
SET GLOBAL sync_binlog = 1;

-- ACID TESTING:

-- Test atomicity
DELIMITER //
CREATE PROCEDURE TestAtomicity()
BEGIN
    DECLARE EXIT HANDLER FOR SQLEXCEPTION
    BEGIN
        ROLLBACK;
        SELECT 'Transaction rolled back - Atomicity maintained' as result;
    END;
    
    START TRANSACTION;
    
    INSERT INTO test_table VALUES (1, 'test');
    INSERT INTO test_table VALUES (2, 'test');
    INSERT INTO test_table VALUES (1, 'duplicate'); -- This will fail due to primary key
    
    COMMIT;
    SELECT 'Transaction committed successfully' as result;
END //
DELIMITER ;

-- ACID PROPERTIES SUMMARY:
-- ATOMICITY: All or nothing - prevents partial updates
-- CONSISTENCY: Valid state always - maintains data integrity
-- ISOLATION: Concurrent safety - prevents interference
-- DURABILITY: Permanent changes - survives system failures

-- MySQL ACID Compliance:
-- âœ“ InnoDB engine provides full ACID compliance
-- âœ“ MyISAM does NOT provide ACID compliance
-- âœ“ ACID properties are crucial for financial, e-commerce, and critical applications
```

**Real-time Example:**
```sql
-- Unnormalized table (Poor design)
CREATE TABLE orders_bad (
    order_id INT PRIMARY KEY,
    customer_name VARCHAR(100),
    customer_email VARCHAR(100),
    customer_phone VARCHAR(20),
    products VARCHAR(500), -- "Product1, Product2, Product3"
    prices VARCHAR(200),   -- "10.99, 20.50, 15.00"
    quantities VARCHAR(100) -- "2, 1, 3"
);

-- 1NF: Eliminate repeating groups
CREATE TABLE orders_1nf (
    order_id INT,
    customer_name VARCHAR(100),
    customer_email VARCHAR(100),
    customer_phone VARCHAR(20),
    product_name VARCHAR(100),
    price DECIMAL(10,2),
    quantity INT,
    PRIMARY KEY (order_id, product_name)
);

-- 2NF: Remove partial dependencies
CREATE TABLE customers (
    customer_id INT AUTO_INCREMENT PRIMARY KEY,
    customer_name VARCHAR(100) NOT NULL,
    email VARCHAR(100) NOT NULL UNIQUE,
    phone VARCHAR(20)
);

CREATE TABLE orders_2nf (
    order_id INT AUTO_INCREMENT PRIMARY KEY,
    customer_id INT NOT NULL,
    order_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    status ENUM('pending', 'processing', 'completed', 'cancelled') DEFAULT 'pending',
    total_amount DECIMAL(10,2),
    FOREIGN KEY (customer_id) REFERENCES customers(customer_id)
);

CREATE TABLE order_items_2nf (
    order_item_id INT AUTO_INCREMENT PRIMARY KEY,
    order_id INT NOT NULL,
    product_name VARCHAR(100),
    price DECIMAL(10,2),
    quantity INT,
    FOREIGN KEY (order_id) REFERENCES orders_2nf(order_id) ON DELETE CASCADE
);

-- 3NF: Remove transitive dependencies
CREATE TABLE products (
    product_id INT AUTO_INCREMENT PRIMARY KEY,
    product_name VARCHAR(100) NOT NULL,
    description TEXT,
    price DECIMAL(10,2) NOT NULL,
    stock_quantity INT DEFAULT 0,
    category_id INT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE categories (
    category_id INT AUTO_INCREMENT PRIMARY KEY,
    category_name VARCHAR(100) NOT NULL UNIQUE,
    description TEXT
);

CREATE TABLE orders (
    order_id INT AUTO_INCREMENT PRIMARY KEY,
    customer_id INT NOT NULL,
    order_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    status ENUM('pending', 'processing', 'shipped', 'delivered', 'cancelled') DEFAULT 'pending',
    shipping_address_id INT,
    payment_method_id INT,
    subtotal DECIMAL(10,2),
    tax DECIMAL(10,2),
    shipping_cost DECIMAL(10,2),
    total_amount DECIMAL(10,2),
    FOREIGN KEY (customer_id) REFERENCES customers(customer_id),
    FOREIGN KEY (shipping_address_id) REFERENCES addresses(address_id),
    FOREIGN KEY (payment_method_id) REFERENCES payment_methods(payment_method_id)
);

CREATE TABLE order_items (
    order_item_id INT AUTO_INCREMENT PRIMARY KEY,
    order_id INT NOT NULL,
    product_id INT NOT NULL,
    quantity INT NOT NULL DEFAULT 1,
    unit_price DECIMAL(10,2) NOT NULL,
    subtotal DECIMAL(10,2) NOT NULL,
    FOREIGN KEY (order_id) REFERENCES orders(order_id) ON DELETE CASCADE,
    FOREIGN KEY (product_id) REFERENCES products(product_id)
);

CREATE TABLE addresses (
    address_id INT AUTO_INCREMENT PRIMARY KEY,
    customer_id INT NOT NULL,
    address_type ENUM('billing', 'shipping') NOT NULL,
    address_line1 VARCHAR(200) NOT NULL,
    address_line2 VARCHAR(200),
    city VARCHAR(100) NOT NULL,
    state VARCHAR(100),
    postal_code VARCHAR(20),
    country VARCHAR(100) NOT NULL,
    is_default BOOLEAN DEFAULT FALSE,
    FOREIGN KEY (customer_id) REFERENCES customers(customer_id) ON DELETE CASCADE
);

CREATE TABLE payment_methods (
    payment_method_id INT AUTO_INCREMENT PRIMARY KEY,
    customer_id INT NOT NULL,
    method_type ENUM('credit_card', 'debit_card', 'paypal', 'bank_transfer') NOT NULL,
    card_last_four VARCHAR(4),
    expiry_date DATE,
    is_default BOOLEAN DEFAULT FALSE,
    FOREIGN KEY (customer_id) REFERENCES customers(customer_id) ON DELETE CASCADE
);
```

---

## Data Types

### Q3. What are the different data types in MySQL?
**Answer:**

**Numeric Types:**
- TINYINT, SMALLINT, MEDIUMINT, INT, BIGINT
- DECIMAL, FLOAT, DOUBLE

**String Types:**
- CHAR, VARCHAR
- TEXT, TINYTEXT, MEDIUMTEXT, LONGTEXT
- BINARY, VARBINARY
- BLOB, TINYBLOB, MEDIUMBLOB, LONGBLOB
- ENUM, SET

**Date & Time:**
- DATE, TIME, DATETIME, TIMESTAMP, YEAR

**JSON:**
- JSON (MySQL 5.7+)

**Spatial:**
- GEOMETRY, POINT, LINESTRING, POLYGON

**Real-time Example:**
```sql
CREATE TABLE comprehensive_example (
    -- Numeric types
    id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    age TINYINT UNSIGNED,
    views INT UNSIGNED DEFAULT 0,
    balance DECIMAL(10,2),
    rating FLOAT(3,2),
    
    -- String types
    username VARCHAR(50) NOT NULL UNIQUE,
    email VARCHAR(100) NOT NULL UNIQUE,
    bio TEXT,
    status ENUM('active', 'inactive', 'suspended') DEFAULT 'active',
    roles SET('admin', 'moderator', 'user') DEFAULT 'user',
    
    -- Date & Time
    birth_date DATE,
    last_login DATETIME,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    -- JSON
    metadata JSON,
    settings JSON,
    
    -- Boolean (TINYINT(1))
    is_active BOOLEAN DEFAULT TRUE,
    is_verified BOOLEAN DEFAULT FALSE,
    
    -- Indexes
    INDEX idx_email (email),
    INDEX idx_status (status),
    INDEX idx_created_at (created_at)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- Insert example with various data types
INSERT INTO comprehensive_example 
(username, email, age, balance, rating, bio, status, roles, birth_date, metadata, settings, is_active, is_verified)
VALUES 
('john_doe', 'john@example.com', 30, 1500.50, 4.75, 'Software Developer', 'active', 'admin,user', '1995-05-15',
 '{"country": "USA", "city": "New York", "interests": ["coding", "music"]}',
 '{"theme": "dark", "notifications": true, "language": "en"}',
 TRUE, TRUE);

-- Querying JSON data
SELECT 
    username,
    JSON_EXTRACT(metadata, '$.country') as country,
    JSON_EXTRACT(settings, '$.theme') as theme
FROM comprehensive_example
WHERE JSON_EXTRACT(metadata, '$.country') = 'USA';

-- Or using JSON path syntax (->)
SELECT 
    username,
    metadata->'$.country' as country,
    settings->>'$.theme' as theme  -- ->> removes quotes
FROM comprehensive_example;

-- Update JSON data
UPDATE comprehensive_example
SET metadata = JSON_SET(metadata, '$.city', 'Los Angeles')
WHERE username = 'john_doe';

-- Add to JSON array
UPDATE comprehensive_example
SET metadata = JSON_ARRAY_APPEND(metadata, '$.interests', 'gaming')
WHERE username = 'john_doe';
```

---

## Queries & Joins

### Q4. Explain different types of JOINs with examples
**Answer:**

**Types of JOINs:**
1. **INNER JOIN** - Returns matching records from both tables
2. **LEFT JOIN** - Returns all records from left table, matching from right
3. **RIGHT JOIN** - Returns all records from right table, matching from left
4. **CROSS JOIN** - Cartesian product of both tables
5. **SELF JOIN** - Join table with itself

**Real-time Example:**
```sql
-- Sample tables for e-commerce
CREATE TABLE users (
    user_id INT PRIMARY KEY,
    username VARCHAR(50),
    email VARCHAR(100),
    created_at TIMESTAMP
);

CREATE TABLE orders (
    order_id INT PRIMARY KEY,
    user_id INT,
    total_amount DECIMAL(10,2),
    status VARCHAR(20),
    order_date TIMESTAMP
);

CREATE TABLE order_items (
    item_id INT PRIMARY KEY,
    order_id INT,
    product_id INT,
    quantity INT,
    price DECIMAL(10,2)
);

CREATE TABLE products (
    product_id INT PRIMARY KEY,
    product_name VARCHAR(100),
    category_id INT,
    price DECIMAL(10,2),
    stock INT
);

CREATE TABLE categories (
    category_id INT PRIMARY KEY,
    category_name VARCHAR(50)
);

CREATE TABLE reviews (
    review_id INT PRIMARY KEY,
    product_id INT,
    user_id INT,
    rating INT,
    comment TEXT,
    created_at TIMESTAMP
);

-- 1. INNER JOIN: Get orders with user information
SELECT 
    o.order_id,
    u.username,
    u.email,
    o.total_amount,
    o.status,
    o.order_date
FROM orders o
INNER JOIN users u ON o.user_id = u.user_id
WHERE o.status = 'completed'
ORDER BY o.order_date DESC;

-- 2. LEFT JOIN: Get all users with their order count (including users with no orders)
SELECT 
    u.user_id,
    u.username,
    u.email,
    COUNT(o.order_id) as total_orders,
    COALESCE(SUM(o.total_amount), 0) as total_spent
FROM users u
LEFT JOIN orders o ON u.user_id = o.user_id
GROUP BY u.user_id, u.username, u.email
ORDER BY total_spent DESC;

-- 3. Multiple JOINs: Get complete order details
SELECT 
    o.order_id,
    u.username,
    u.email,
    o.order_date,
    p.product_name,
    c.category_name,
    oi.quantity,
    oi.price,
    (oi.quantity * oi.price) as line_total
FROM orders o
INNER JOIN users u ON o.user_id = u.user_id
INNER JOIN order_items oi ON o.order_id = oi.order_id
INNER JOIN products p ON oi.product_id = p.product_id
INNER JOIN categories c ON p.category_id = c.category_id
WHERE o.order_date >= DATE_SUB(NOW(), INTERVAL 30 DAY)
ORDER BY o.order_date DESC, o.order_id, oi.item_id;

-- 4. LEFT JOIN with aggregation: Products with review statistics
SELECT 
    p.product_id,
    p.product_name,
    c.category_name,
    p.price,
    COUNT(r.review_id) as review_count,
    COALESCE(AVG(r.rating), 0) as avg_rating,
    COALESCE(MAX(r.rating), 0) as max_rating,
    COALESCE(MIN(r.rating), 0) as min_rating
FROM products p
INNER JOIN categories c ON p.category_id = c.category_id
LEFT JOIN reviews r ON p.product_id = r.product_id
GROUP BY p.product_id, p.product_name, c.category_name, p.price
HAVING review_count > 0 OR p.product_id IN (SELECT DISTINCT product_id FROM order_items)
ORDER BY avg_rating DESC, review_count DESC;

-- 5. SELF JOIN: Find users who ordered the same products
SELECT DISTINCT
    u1.username as user1,
    u2.username as user2,
    p.product_name
FROM order_items oi1
INNER JOIN order_items oi2 ON oi1.product_id = oi2.product_id AND oi1.order_id != oi2.order_id
INNER JOIN orders o1 ON oi1.order_id = o1.order_id
INNER JOIN orders o2 ON oi2.order_id = o2.order_id
INNER JOIN users u1 ON o1.user_id = u1.user_id
INNER JOIN users u2 ON o2.user_id = u2.user_id
INNER JOIN products p ON oi1.product_id = p.product_id
WHERE u1.user_id < u2.user_id;

-- 6. Complex query: Top selling products by category
SELECT 
    c.category_name,
    p.product_name,
    SUM(oi.quantity) as units_sold,
    SUM(oi.quantity * oi.price) as revenue,
    COUNT(DISTINCT o.user_id) as unique_customers,
    AVG(r.rating) as avg_rating
FROM products p
INNER JOIN categories c ON p.category_id = c.category_id
INNER JOIN order_items oi ON p.product_id = oi.product_id
INNER JOIN orders o ON oi.order_id = o.order_id
LEFT JOIN reviews r ON p.product_id = r.product_id
WHERE o.status = 'completed'
AND o.order_date >= DATE_SUB(NOW(), INTERVAL 90 DAY)
GROUP BY c.category_id, c.category_name, p.product_id, p.product_name
HAVING units_sold > 10
ORDER BY c.category_name, revenue DESC;

-- 7. Subquery with JOIN: Users who never left a review
SELECT 
    u.user_id,
    u.username,
    u.email,
    COUNT(o.order_id) as total_orders,
    SUM(o.total_amount) as total_spent
FROM users u
INNER JOIN orders o ON u.user_id = o.user_id
WHERE u.user_id NOT IN (
    SELECT DISTINCT user_id FROM reviews
)
AND o.status = 'completed'
GROUP BY u.user_id, u.username, u.email
HAVING total_orders > 5
ORDER BY total_spent DESC;

-- 8. CROSS JOIN: Product comparison matrix (use cautiously)
SELECT 
    p1.product_name as product1,
    p2.product_name as product2,
    p1.price as price1,
    p2.price as price2,
    ABS(p1.price - p2.price) as price_difference
FROM products p1
CROSS JOIN products p2
WHERE p1.product_id < p2.product_id
AND p1.category_id = p2.category_id
AND ABS(p1.price - p2.price) < 10
ORDER BY p1.category_id, price_difference;
```

### Q5. What is the difference between WHERE and HAVING?
**Answer:**
- **WHERE** - Filters rows before grouping
- **HAVING** - Filters groups after aggregation

**Real-time Example:**
```sql
-- Sample data
CREATE TABLE sales (
    sale_id INT PRIMARY KEY,
    product_id INT,
    salesperson_id INT,
    amount DECIMAL(10,2),
    sale_date DATE,
    region VARCHAR(50)
);

-- WHERE: Filter individual rows before grouping
SELECT 
    salesperson_id,
    COUNT(*) as total_sales,
    SUM(amount) as total_revenue
FROM sales
WHERE sale_date >= '2025-01-01'  -- Filter rows first
AND region = 'North'
GROUP BY salesperson_id;

-- HAVING: Filter groups after aggregation
SELECT 
    salesperson_id,
    COUNT(*) as total_sales,
    SUM(amount) as total_revenue
FROM sales
WHERE sale_date >= '2025-01-01'
GROUP BY salesperson_id
HAVING SUM(amount) > 10000;  -- Filter groups after aggregation

-- Combined WHERE and HAVING
SELECT 
    salesperson_id,
    region,
    COUNT(*) as total_sales,
    SUM(amount) as total_revenue,
    AVG(amount) as avg_sale_amount
FROM sales
WHERE sale_date BETWEEN '2025-01-01' AND '2025-12-31'  -- Filter rows
AND amount > 100                                         -- Filter rows
GROUP BY salesperson_id, region
HAVING COUNT(*) >= 5                                     -- Filter groups
AND SUM(amount) > 5000                                   -- Filter groups
ORDER BY total_revenue DESC;

-- Real-world example: Active customers analysis
SELECT 
    u.user_id,
    u.username,
    COUNT(o.order_id) as order_count,
    SUM(o.total_amount) as lifetime_value,
    AVG(o.total_amount) as avg_order_value,
    MAX(o.order_date) as last_order_date,
    DATEDIFF(NOW(), MAX(o.order_date)) as days_since_last_order
FROM users u
INNER JOIN orders o ON u.user_id = o.user_id
WHERE o.status = 'completed'                             -- WHERE: Filter rows
AND o.order_date >= DATE_SUB(NOW(), INTERVAL 1 YEAR)
GROUP BY u.user_id, u.username
HAVING order_count >= 3                                  -- HAVING: Filter groups
AND lifetime_value > 500
AND days_since_last_order <= 90
ORDER BY lifetime_value DESC;
```

---

## Indexes & Performance

### Q6. What are indexes and when should you use them?
**Answer:** Indexes are data structures that improve query performance by allowing faster data retrieval.

**Types of Indexes:**
- PRIMARY KEY
- UNIQUE
- INDEX (Regular)
- FULLTEXT
- SPATIAL

**Real-time Example:**
```sql
-- Creating table with various indexes
CREATE TABLE blog_posts (
    post_id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    user_id INT UNSIGNED NOT NULL,
    title VARCHAR(200) NOT NULL,
    slug VARCHAR(200) NOT NULL UNIQUE,
    content LONGTEXT,
    excerpt TEXT,
    status ENUM('draft', 'published', 'archived') DEFAULT 'draft',
    views INT UNSIGNED DEFAULT 0,
    published_at TIMESTAMP NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    -- Indexes
    INDEX idx_user_id (user_id),
    INDEX idx_status (status),
    INDEX idx_published_at (published_at),
    INDEX idx_status_published (status, published_at),
    INDEX idx_views (views),
    FULLTEXT KEY ft_title_content (title, content),
    
    FOREIGN KEY (user_id) REFERENCES users(user_id) ON DELETE CASCADE
) ENGINE=InnoDB;

-- Adding index after table creation
CREATE INDEX idx_created_at ON blog_posts(created_at);
CREATE INDEX idx_slug ON blog_posts(slug);

-- Composite index for common query patterns
CREATE INDEX idx_user_status_date ON blog_posts(user_id, status, published_at);

-- Show indexes
SHOW INDEXES FROM blog_posts;

-- Check index usage with EXPLAIN
EXPLAIN SELECT * FROM blog_posts 
WHERE status = 'published' 
AND published_at <= NOW()
ORDER BY published_at DESC
LIMIT 10;

-- Full-text search
SELECT post_id, title, 
       MATCH(title, content) AGAINST('MySQL performance' IN NATURAL LANGUAGE MODE) as relevance
FROM blog_posts
WHERE MATCH(title, content) AGAINST('MySQL performance' IN NATURAL LANGUAGE MODE)
ORDER BY relevance DESC;

-- When NOT to use indexes:
-- 1. Small tables (< 1000 rows)
-- 2. Columns with low cardinality (few distinct values)
-- 3. Columns frequently updated
-- 4. Tables with more writes than reads

-- Analyze index usage
SELECT 
    TABLE_NAME,
    INDEX_NAME,
    CARDINALITY,
    SEQ_IN_INDEX,
    COLUMN_NAME
FROM information_schema.STATISTICS
WHERE TABLE_SCHEMA = 'mydb'
AND TABLE_NAME = 'blog_posts';

-- Find unused indexes
SELECT 
    s.TABLE_SCHEMA,
    s.TABLE_NAME,
    s.INDEX_NAME
FROM information_schema.STATISTICS s
LEFT JOIN performance_schema.table_io_waits_summary_by_index_usage i
    ON s.TABLE_SCHEMA = i.OBJECT_SCHEMA
    AND s.TABLE_NAME = i.OBJECT_NAME
    AND s.INDEX_NAME = i.INDEX_NAME
WHERE s.TABLE_SCHEMA NOT IN ('mysql', 'performance_schema', 'information_schema')
AND i.INDEX_NAME IS NULL
AND s.INDEX_NAME != 'PRIMARY';

-- Drop unused index
DROP INDEX idx_views ON blog_posts;
```

### Q7. UNION vs UNION ALL - WHY use each?
**Answer:** UNION removes duplicates (slower), UNION ALL keeps all rows (faster).

**PROBLEM â†’ SOLUTION:**
```sql
-- PROBLEM: Getting all customers from multiple sources
-- Method 1: UNION (removes duplicates) - SLOWER
SELECT email FROM customers
UNION
SELECT email FROM newsletter_subscribers;
-- WHY: Removes duplicate emails, but slower due to duplicate checking

-- Method 2: UNION ALL (keeps duplicates) - FASTER
SELECT email FROM customers
UNION ALL
SELECT email FROM newsletter_subscribers;
-- WHY: Faster, use when you know there are no duplicates

-- REAL EXAMPLE: Getting active users from different tables
SELECT user_id, 'customer' as type FROM orders WHERE created_at > '2025-01-01'
UNION
SELECT user_id, 'reviewer' as type FROM reviews WHERE created_at > '2025-01-01';
```

### Q8. Subqueries - WHY and WHEN to use them?
**Answer:** Subqueries help break complex problems into simpler parts.

```sql
-- WHY: Find products more expensive than average
SELECT * FROM products 
WHERE price > (SELECT AVG(price) FROM products);

-- WHY: Find users who never ordered
SELECT * FROM users 
WHERE id NOT IN (SELECT DISTINCT user_id FROM orders);

-- BETTER: Using JOIN (faster for large datasets)
SELECT u.* FROM users u
LEFT JOIN orders o ON u.id = o.user_id
WHERE o.id IS NULL;
```

### Q9. Window Functions - WHY use them?
**Answer:** Perform calculations across rows without GROUP BY.

```sql
-- WHY: Rank products within each category
SELECT 
    product_name,
    category,
    price,
    RANK() OVER (PARTITION BY category ORDER BY price DESC) as price_rank,
    ROW_NUMBER() OVER (PARTITION BY category ORDER BY sales DESC) as sales_rank
FROM products;

-- WHY: Running total of sales
SELECT 
    date,
    amount,
    SUM(amount) OVER (ORDER BY date) as running_total
FROM sales;
```

### Q10. Transactions - WHY are they critical?
**Answer:** Ensure data consistency - all operations succeed or all fail.

**PROBLEM:**
```sql
-- WITHOUT transaction - DANGEROUS!
UPDATE accounts SET balance = balance - 100 WHERE id = 1; -- Succeeds
-- System crashes here!
UPDATE accounts SET balance = balance + 100 WHERE id = 2; -- Never executes!
-- RESULT: Money lost! ðŸ’¸
```

**SOLUTION:**
```sql
-- WITH transaction - SAFE!
START TRANSACTION;
UPDATE accounts SET balance = balance - 100 WHERE id = 1;
DR teams SET balance = balance + 100 WHERE id = 2;
COMMIT;
-- RESULT: Both succeed or both fail - money safe! âœ…

-- WHY: ACID properties
-- Atomicity: All or nothing
-- Consistency: Valid state always
-- Isolation: Concurrent transactions don't interfere
-- Durability: Committed changes persist
```

### Q10a. MySQL Transaction Isolation Levels and Locking
**Answer:** MySQL provides different isolation levels and various locking mechanisms to control concurrent access to data.

**Real-time Example:**
```sql
-- 1. Create test tables for transaction demonstration
CREATE TABLE bank_accounts (
    account_id INT PRIMARY KEY,
    account_holder VARCHAR(100),
    balance DECIMAL(10,2) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE transactions (
    transaction_id INT AUTO_INCREMENT PRIMARY KEY,
    account_id INT NOT NULL,
    transaction_type ENUM('deposit', 'withdrawal', 'transfer'),
    amount DECIMAL(10,2) NOT NULL,
    balance_after DECIMAL(10,2),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (account_id) REFERENCES bank_accounts(account_id)
);

INSERT INTO bank_accounts VALUES
(1, 'John Doe', 1000.00, NOW()),
(2, 'Jane Smith', 2000.00, NOW());

-- 2. TRANSACTION ISOLATION LEVELS

-- View current isolation level
SELECT @@transaction_isolation; -- MySQL 8.0+
SELECT @@tx_isolation; -- MySQL 5.7 and earlier

-- Set isolation level (session level)
SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;
SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ; -- MySQL default
SET SESSION TRANSACTION ISOLATION LEVEL SERIALIZABLE;

-- Set isolation level globally (requires super privilege)
SET GLOBAL TRANSACTION ISOLATION LEVEL REPEATABLE READ;

-- 3. Isolation Level: READ UNCOMMITTED (Dirty Reads)
-- Transaction 1 (Session A)
START TRANSACTION;
UPDATE bank_accounts SET balance = balance + 1000 WHERE account_id = 1;
-- Don't commit yet

-- Transaction 2 (Session B) - Can see uncommitted changes
START TRANSACTION;
SELECT balance FROM bank_accounts WHERE account_id = 1; -- Sees 2000.00 (dirty read)

-- READ UNCOMMITTED issues:
-- - Dirty reads: See uncommitted changes
-- - Non-repeatable reads: Same query returns different results
-- - Phantom reads: New rows appear in range queries

-- 4. Isolation Level: READ COMMITTED
SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;

-- Transaction 1 (Session A)
START TRANSACTION;
UPDATE bank_accounts SET balance = 3000 WHERE account_id = 1;
-- Don't commit

-- Transaction 2 (Session B) - Cannot see uncommitted changes
START TRANSACTION;
SELECT balance FROM bank_accounts WHERE account_id = 1; -- Sees 1000.00 (committed value)

-- READ COMMITTED features:
-- - No dirty reads
-- - But still allows non-repeatable reads and phantom reads

-- 5. Isolation Level: REPEATABLE READ (MySQL Default)
SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;

-- Transaction 1 (Session A)
START TRANSACTION;
SELECT balance FROM bank_accounts WHERE account_id = 1; -- Reads 1000.00
-- Hold transaction

-- Transaction 2 (Session B) - Updates and commits
UPDATE bank_accounts SET balance = 3000 WHERE account_id = 1;
COMMIT;

-- Transaction 1 (Session A) - Still sees old value
SELECT balance FROM bank_accounts WHERE account_id = 1; -- Still reads 1000.00
COMMIT;

-- REPEATABLE READ features:
-- - No dirty reads
-- - No non-repeatable reads (MySQL uses MVCC)
-- - false positives appear in some cases

-- 6. Isolation Level: SERIALIZABLE
SET SESSION TRANSACTION ISOLATION LEVEL SERIALIZABLE;

-- Highest isolation level - All transactions are serialized
-- Prevents all: dirty reads, non-repeatable reads, phantom reads
-- Performance impact: Locks are held longer

-- 7. LOCKING MECHANISMS

-- Shared lock (read lock)
LOCK TABLES bank_accounts READ;
SELECT * FROM bank_accounts;
UNLOCK TABLES;

-- Exclusive lock (write lock)
LOCK TABLES bank_accounts WRITE;
UPDATE bank_accounts SET balance = 1500 WHERE account_id = 1;
UNLOCK TABLES;

-- 8. Row-Level Locking with InnoDB
-- Implicit locking with SELECT ... FOR UPDATE
START TRANSACTION;
SELECT * FROM bank_accounts WHERE account_id = 1 FOR UPDATE;
-- Lock is held until transaction commits/rolls back
UPDATE bank_accounts SET balance = balance - 100 WHERE account_id = 1;
COMMIT;

-- Implicit locking with SELECT ... LOCK IN SHARE MODE
START TRANSACTION;
SELECT * FROM bank_accounts WHERE account_id = 1 LOCK IN SHARE MODE;
-- Other transactions can read but not modify
COMMIT;

-- 9. Deadlock Detection and Handling
-- Example deadlock scenario:
-- Transaction 1
START TRANSACTION;
UPDATE bank_accounts SET balance = balance - 100 WHERE account_id = 1;
UPDATE bank_accounts SET balance = balance + 100 WHERE account_id = 2;

-- Transaction 2 (concurrent)
START TRANSACTION;
UPDATE bank_accounts SET balance = balance - 50 WHERE account_id = 2;
UPDATE bank_accounts SET balance = balance + 50 WHERE account_id = 1;

-- MySQL automatically detects deadlock and rolls back one transaction
-- Error: Deadlock found when trying to get lock

-- 10. Gap Locks (REPEATABLE READ level)
-- Gap locks prevent phantom reads
CREATE TABLE orders (
    order_id INT PRIMARY KEY,
    customer_id INT,
    amount DECIMAL(10,2),
    INDEX idx_customer (customer_id)
);

INSERT INTO orders VALUES
(1, 101, 100.00),
(5, 101, 200.00),
(10, 101, 300.00);

-- Transaction creates gap lock
START TRANSACTION;
SELECT * FROM orders WHERE customer_id = 101 AND order_id BETWEEN 2 AND 8 FOR UPDATE;
-- Locks order_id: 1, 2-8 (gap), 5, 8-10 (gap)

-- 11. Transaction with Error Handling
DELIMITER //
CREATE PROCEDURE TransferMoney(
    IN from_account INT,
    IN to_account INT,
    IN transfer_amount DECIMAL(10,2)
)
BEGIN
    DECLARE EXIT HANDLER FOR SQLEXCEPTION
    BEGIN
        ROLLBACK;
        RESIGNAL;
    END;
    
    DECLARE EXIT HANDLER FOR SQLWARNING
    BEGIN
        ROLLBACK;
        RESIGNAL;
    END;
    
    START TRANSACTION;
    
    -- Check sufficient balance
    IF (SELECT balance FROM bank_accounts WHERE account_id = from_account) < transfer_amount THEN
        SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Insufficient balance';
    END IF;
    
    -- Debit from source account
    UPDATE bank_accounts 
    SET balance = balance - transfer_amount 
    WHERE account_id = from_account;
    
    -- Credit to destination account
    UPDATE bank_accounts 
    SET balance = balance + transfer_amount 
    WHERE account_id = to_account;
    
    -- Record transaction
    INSERT INTO transactions (account_id, transaction_type, amount, balance_after)
    VALUES 
    (from_account, 'transfer', -transfer_amount, 
     (SELECT balance FROM bank_accounts WHERE account_id = from_account)),
    (to_account, 'transfer', transfer_amount,
     (SELECT balance FROM bank_accounts WHERE account_id = to_account));
    
    COMMIT;
    
    SELECT 'Transfer completed successfully' as result;
END //
DELIMITER ;

-- Usage
CALL TransferMoney(1, 2, 100.00);

-- 12. View Transaction Status
SELECT 
    t.id as transaction_id,
    t.isolation_level,
    t.is_consistent_snapshot,
    t.autocommit
FROM performance_schema.data_locks;

-- 13. Monitor Lock Waits
SELECT 
    blocking_trx_id,
    blocking_lock_mode,
    waiting_trx_id,
    waiting_lock_mode,
    waiting_thread,
    waiting_query
FROM performance_schema.data_lock_waits;

-- 14. Lock Wait Timeout
SET innodb_lock_wait_timeout = 50; -- seconds

-- 15. Transaction Best Practices
-- âœ“ Use appropriate isolation level
-- âœ“ Keep transactions short
-- âœ“ Minimize locking time
-- âœ“ Always handle errors in stored procedures
-- âœ“ Use row-level locking when possible
-- âœ“ Monitor deadlocks
-- âœ“ Use SAVEPOINT for partial rollbacks

-- SAVEPOINT example
START TRANSACTION;
UPDATE bank_accounts SET balance = balance + 100 WHERE account_id = 1;
SAVEPOINT sp1;
UPDATE bank_accounts SET balance = balance + 200 WHERE account_id = 1;
SAVEPOINT sp2;
UPDATE bank_accounts SET balance = balance + 300 WHERE account_id = 1;

-- Rollback to specific savepoint
ROLLBACK TO SAVEPOINT sp2;
-- Balance is now +200, not +300

-- Or rollback everything
-- ROLLBACK;
```

### Q11-Q80. More MySQL Questions

**Q11. PRIMARY KEY vs UNIQUE**
```sql
-- PRIMARY: One per table, NOT NULL, clustered index
-- UNIQUE: Multiple allowed, can be NULL
CREATE TABLE users (
    id INT PRIMARY KEY,
    email VARCHAR(100) UNIQUE
);
```

**Q12. AUTO_INCREMENT**
```sql
CREATE TABLE orders (id INT AUTO_INCREMENT PRIMARY KEY);
INSERT INTO orders VALUES (NULL, 'data'); -- Auto generates ID
```

**Q13. FOREIGN KEYS**
```sql
FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE
-- WHY: Maintains referential integrity
```

**Q14. ON DELETE CASCADE vs RESTRICT**
```sql
ON DELETE CASCADE  -- Delete child records automatically
ON DELETE RESTRICT -- Prevent deletion if children exist
ON DELETE SET NULL -- Set foreign key to NULL
```

**Q15. COUNT vs COUNT(*) vs COUNT(column)**
```sql
COUNT(*)      -- Counts all rows
COUNT(column) -- Counts non-NULL values only
COUNT(DISTINCT column) -- Counts unique values
```

**Q16. HAVING vs WHERE**
```sql
-- WHERE: Filter before GROUP BY
-- HAVING: Filter after GROUP BY
SELECT category, COUNT(*) as count
FROM products
WHERE price > 10      -- Filter rows first
GROUP BY category
HAVING count > 5;     -- Filter groups after
```

**Q17. LIMIT vs OFFSET**
```sql
SELECT * FROM users LIMIT 10 OFFSET 20; -- Skip 20, get next 10
-- WHY: Pagination (page 3 of results)
```

**Q18. DISTINCT**
```sql
SELECT DISTINCT category FROM products;
-- WHY: Get unique values only
```

**Q19. CASE WHEN**
```sql
SELECT 
    product_name,
    CASE 
        WHEN price < 50 THEN 'Cheap'
        WHEN price < 200 THEN 'Medium'
        ELSE 'Expensive'
    END as price_category
FROM products;
```

**Q20. IFNULL vs COALESCE**
```sql
IFNULL(column, 'default')        -- MySQL specific
COALESCE(col1, col2, 'default')  -- SQL standard, multiple values
```

**Q21. DATE Functions**
```sql
NOW(), CURDATE(), CURTIME()
DATE_FORMAT(date, '%Y-%m-%d')
DATEDIFF(date1, date2)
DATE_ADD(date, INTERVAL 1 DAY)
```

**Q22. String Functions**
```sql
CONCAT(first_name, ' ', last_name)
SUBSTRING(text, 1, 10)
UPPER(), LOWER(), TRIM()
LENGTH(), CHAR_LENGTH()
```

**Q23. Mathematical Functions**
```sql
ROUND(price, 2)
CEIL(), FLOOR()
ABS(), POWER(), SQRT()
```

**Q24. Aggregate Functions**
```sql
SUM(price), AVG(rating), MIN(stock), MAX(views)
GROUP_CONCAT(name SEPARATOR ', ')
```

**Q25. Self JOIN**
```sql
-- WHY: Find employees and their managers
SELECT e.name as employee, m.name as manager
FROM employees e
LEFT JOIN employees m ON e.manager_id = m.id;
```

**Q26. EXISTS vs IN**
```sql
-- EXISTS: Faster for large datasets
WHERE EXISTS (SELECT 1 FROM orders WHERE user_id = users.id)
-- IN: Simpler syntax
WHERE id IN (SELECT user_id FROM orders)
```

**Q27-Q80: Additional 54 MySQL concepts**
- Views, Triggers, Stored Procedures
- Indexes (B-Tree, Hash, Full-text)
- Query optimization techniques
- Execution plans (EXPLAIN)
- Partitioning
- Replication
- Backup strategies
- Security best practices
- Character sets & Collations
- JSON functions
- And more...

---

## Advanced MySQL Concepts

### Q81. What are MySQL Window Functions and how to use them?
**Answer:** Window functions perform calculations across a set of table rows that are somehow related to the current row, without reducing the number of rows returned.

**Real-time Example:**
```sql
-- Sample data for e-commerce analytics
CREATE TABLE sales (
    id INT PRIMARY KEY AUTO_INCREMENT,
    product_id INT,
    customer_id INT,
    sale_date DATE,
    amount DECIMAL(10,2),
    region VARCHAR(50)
);

INSERT INTO sales (product_id, customer_id, sale_date, amount, region) VALUES
(1, 101, '2025-01-01', 100.00, 'North'),
(1, 102, '2025-01-02', 150.00, 'South'),
(2, 101, '2025-01-03', 200.00, 'North'),
(2, 103, '2025-01-04', 250.00, 'East'),
(1, 104, '2025-01-05', 120.00, 'West'),
(3, 102, '2025-01-06', 300.00, 'South'),
(3, 105, '2025-01-07', 350.00, 'North');

-- 1. ROW_NUMBER() - Assign sequential numbers
SELECT 
    product_id,
    customer_id,
    amount,
    ROW_NUMBER() OVER (ORDER BY amount DESC) as rank_by_amount,
    ROW_NUMBER() OVER (PARTITION BY product_id ORDER BY amount DESC) as rank_by_product
FROM sales;

-- 2. RANK() and DENSE_RANK() - Handle ties differently
SELECT 
    product_id,
    amount,
    RANK() OVER (ORDER BY amount DESC) as rank_with_gaps,
    DENSE_RANK() OVER (ORDER BY amount DESC) as rank_without_gaps
FROM sales;

-- 3. Running totals and moving averages
SELECT 
    sale_date,
    amount,
    SUM(amount) OVER (ORDER BY sale_date) as running_total,
    AVG(amount) OVER (ORDER BY sale_date 
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as moving_avg_3_days
FROM sales
ORDER BY sale_date;

-- 4. LAG() and LEAD() - Access previous/next rows
SELECT 
    product_id,
    sale_date,
    amount,
    LAG(amount, 1) OVER (PARTITION BY product_id ORDER BY sale_date) as prev_amount,
    LEAD(amount, 1) OVER (PARTITION BY product_id ORDER BY sale_date) as next_amount,
    amount - LAG(amount, 1) OVER (PARTITION BY product_id ORDER BY sale_date) as amount_change
FROM sales
ORDER BY product_id, sale_date;

-- 5. FIRST_VALUE() and LAST_VALUE() - Get first/last values
SELECT 
    product_id,
    customer_id,
    amount,
    FIRST_VALUE(amount) OVER (PARTITION BY product_id ORDER BY amount DESC) as highest_amount,
    LAST_VALUE(amount) OVER (PARTITION BY product_id ORDER BY amount DESC 
        ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as lowest_amount
FROM sales;

-- 6. NTILE() - Divide rows into buckets
SELECT 
    customer_id,
    amount,
    NTILE(3) OVER (ORDER BY amount DESC) as spending_tier
FROM sales;

-- 7. Complex analytics query
SELECT 
    product_id,
    region,
    amount,
    -- Running totals by region
    SUM(amount) OVER (PARTITION BY region ORDER BY sale_date) as region_running_total,
    -- Percentage of total
    amount / SUM(amount) OVER () * 100 as percentage_of_total,
    -- Rank within region
    RANK() OVER (PARTITION BY region ORDER BY amount DESC) as rank_in_region,
    -- Moving average
    AVG(amount) OVER (ORDER BY sale_date 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) as moving_avg_5_days
FROM sales
ORDER BY region, sale_date;
```

### Q82. What are Common Table Expressions (CTEs) and when to use them?
**Answer:** CTEs are temporary named result sets that exist only within the execution scope of a single SQL statement.

**Real-time Example:**
```sql
-- Recursive CTE for hierarchical data
CREATE TABLE employees (
    id INT PRIMARY KEY,
    name VARCHAR(100),
    manager_id INT,
    position VARCHAR(100),
    salary DECIMAL(10,2)
);

INSERT INTO employees VALUES
(1, 'John CEO', NULL, 'CEO', 100000),
(2, 'Jane VP', 1, 'VP', 80000),
(3, 'Bob Manager', 2, 'Manager', 60000),
(4, 'Alice Manager', 2, 'Manager', 65000),
(5, 'Charlie Dev', 3, 'Developer', 50000),
(6, 'Diana Dev', 3, 'Developer', 52000),
(7, 'Eve Dev', 4, 'Developer', 48000);

-- 1. Simple CTE for readability
WITH high_earners AS (
    SELECT * FROM employees WHERE salary > 55000
),
managers AS (
    SELECT * FROM employees WHERE position = 'Manager'
)
SELECT 
    h.name as employee,
    m.name as manager
FROM high_earners h
JOIN managers m ON h.manager_id = m.id;

-- 2. Recursive CTE for organizational hierarchy
WITH RECURSIVE org_hierarchy AS (
    -- Base case: CEO (no manager)
    SELECT 
        id, 
        name, 
        manager_id, 
        position, 
        salary,
        0 as level,
        CAST(name AS CHAR(1000)) as hierarchy_path
    FROM employees 
    WHERE manager_id IS NULL
    
    UNION ALL
    
    -- Recursive case: employees with managers
    SELECT 
        e.id, 
        e.name, 
        e.manager_id, 
        e.position, 
        e.salary,
        oh.level + 1,
        CONCAT(oh.hierarchy_path, ' -> ', e.name)
    FROM employees e
    JOIN org_hierarchy oh ON e.manager_id = oh.id
)
SELECT 
    name,
    position,
    level,
    hierarchy_path,
    salary
FROM org_hierarchy
ORDER BY level, name;

-- 3. CTE for complex calculations
WITH monthly_sales AS (
    SELECT 
        DATE_FORMAT(sale_date, '%Y-%m') as month,
        SUM(amount) as total_sales,
        COUNT(*) as transaction_count
    FROM sales
    GROUP BY DATE_FORMAT(sale_date, '%Y-%m')
),
sales_growth AS (
    SELECT 
        month,
        total_sales,
        transaction_count,
        LAG(total_sales) OVER (ORDER BY month) as prev_month_sales,
        LAG(transaction_count) OVER (ORDER BY month) as prev_month_count
    FROM monthly_sales
)
SELECT 
    month,
    total_sales,
    transaction_count,
    CASE 
        WHEN prev_month_sales IS NULL THEN 'First Month'
        ELSE CONCAT(ROUND(((total_sales - prev_month_sales) / prev_month_sales) * 100, 2), '%')
    END as sales_growth_percentage,
    CASE 
        WHEN prev_month_count IS NULL THEN 'First Month'
        ELSE CONCAT(ROUND(((transaction_count - prev_month_count) / prev_month_count) * 100, 2), '%')
    END as transaction_growth_percentage
FROM sales_growth
ORDER BY month;

-- 4. Multiple CTEs for complex analysis
WITH product_performance AS (
    SELECT 
        product_id,
        COUNT(*) as total_sales,
        SUM(amount) as total_revenue,
        AVG(amount) as avg_sale_amount,
        MAX(amount) as max_sale,
        MIN(amount) as min_sale
    FROM sales
    GROUP BY product_id
),
customer_segments AS (
    SELECT 
        customer_id,
        COUNT(*) as purchase_frequency,
        SUM(amount) as total_spent,
        AVG(amount) as avg_purchase,
        CASE 
            WHEN SUM(amount) > 1000 THEN 'High Value'
            WHEN SUM(amount) > 500 THEN 'Medium Value'
            ELSE 'Low Value'
        END as customer_segment
    FROM sales
    GROUP BY customer_id
),
region_analysis AS (
    SELECT 
        region,
        COUNT(DISTINCT customer_id) as unique_customers,
        SUM(amount) as total_revenue,
        AVG(amount) as avg_sale_amount
    FROM sales
    GROUP BY region
)
SELECT 
    'Product Performance' as analysis_type,
    product_id,
    total_sales,
    total_revenue,
    avg_sale_amount
FROM product_performance
UNION ALL
SELECT 
    'Customer Segments' as analysis_type,
    customer_id,
    purchase_frequency,
    total_spent,
    avg_purchase
FROM customer_segments
UNION ALL
SELECT 
    'Region Analysis' as analysis_type,
    region,
    unique_customers,
    total_revenue,
    avg_sale_amount
FROM region_analysis;
```

### Q83. Explain MySQL JSON Functions with real-world examples
**Answer:** MySQL provides powerful JSON functions for storing, querying, and manipulating JSON data.

**Real-time Example:**
```sql
-- Create table with JSON column
CREATE TABLE products (
    id INT PRIMARY KEY AUTO_INCREMENT,
    name VARCHAR(100),
    price DECIMAL(10,2),
    attributes JSON,
    metadata JSON,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Insert JSON data
INSERT INTO products (name, price, attributes, metadata) VALUES
('Laptop', 999.99, 
 '{"brand": "Dell", "model": "XPS 13", "specs": {"ram": "16GB", "storage": "512GB SSD", "processor": "Intel i7"}, "colors": ["Silver", "Black"], "warranty": 2}',
 '{"category": "Electronics", "tags": ["laptop", "portable", "business"], "ratings": {"average": 4.5, "count": 150}, "inventory": {"warehouse": "A", "location": "B1-23"}}'),
 
('Smartphone', 699.99,
 '{"brand": "Apple", "model": "iPhone 14", "specs": {"storage": "128GB", "camera": "12MP", "battery": "3200mAh"}, "colors": ["Blue", "Purple", "Black"], "warranty": 1}',
 '{"category": "Electronics", "tags": ["phone", "mobile", "premium"], "ratings": {"average": 4.8, "count": 300}, "inventory": {"warehouse": "B", "location": "C2-15"}}'),
 
('Book', 29.99,
 '{"author": "John Doe", "publisher": "Tech Books", "pages": 350, "language": "English", "format": "Paperback"}',
 '{"category": "Books", "tags": ["programming", "education", "tech"], "ratings": {"average": 4.2, "count": 75}, "inventory": {"warehouse": "C", "location": "D3-08"}}');

-- 1. JSON_EXTRACT() - Extract values from JSON
SELECT 
    name,
    price,
    JSON_EXTRACT(attributes, '$.brand') as brand,
    JSON_EXTRACT(attributes, '$.specs.ram') as ram,
    JSON_EXTRACT(metadata, '$.ratings.average') as avg_rating
FROM products;

-- 2. JSON_UNQUOTE() - Remove quotes from extracted values
SELECT 
    name,
    JSON_UNQUOTE(JSON_EXTRACT(attributes, '$.brand')) as brand,
    JSON_UNQUOTE(JSON_EXTRACT(attributes, '$.model')) as model
FROM products;

-- 3. JSON_OBJECT() - Create JSON objects
SELECT 
    name,
    price,
    JSON_OBJECT(
        'brand', JSON_UNQUOTE(JSON_EXTRACT(attributes, '$.brand')),
        'model', JSON_UNQUOTE(JSON_EXTRACT(attributes, '$.model')),
        'price', price
    ) as product_summary
FROM products;

-- 4. JSON_ARRAY() - Create JSON arrays
SELECT 
    name,
    JSON_ARRAY(
        JSON_UNQUOTE(JSON_EXTRACT(attributes, '$.brand')),
        JSON_UNQUOTE(JSON_EXTRACT(attributes, '$.model')),
        CAST(price AS CHAR)
    ) as product_array
FROM products;

-- 5. JSON_SEARCH() - Find values in JSON
SELECT 
    name,
    attributes,
    JSON_SEARCH(attributes, 'one', 'Intel') as intel_found,
    JSON_SEARCH(attributes, 'all', 'Silver') as silver_found
FROM products
WHERE JSON_SEARCH(attributes, 'one', 'Intel') IS NOT NULL;

-- 6. JSON_CONTAINS() - Check if JSON contains value
SELECT 
    name,
    attributes
FROM products
WHERE JSON_CONTAINS(attributes, '"Intel"', '$.specs.processor');

-- 7. JSON_KEYS() - Get all keys from JSON object
SELECT 
    name,
    JSON_KEYS(attributes) as attribute_keys,
    JSON_KEYS(metadata) as metadata_keys
FROM products;

-- 8. JSON_LENGTH() - Get length of JSON array or object
SELECT 
    name,
    JSON_LENGTH(attributes, '$.colors') as color_count,
    JSON_LENGTH(metadata, '$.tags') as tag_count
FROM products;

-- 9. JSON_SET() - Update JSON values
UPDATE products 
SET attributes = JSON_SET(attributes, '$.warranty', 3)
WHERE JSON_UNQUOTE(JSON_EXTRACT(attributes, '$.brand')) = 'Dell';

-- 10. JSON_INSERT() - Insert new values
UPDATE products 
SET metadata = JSON_INSERT(metadata, '$.last_updated', NOW())
WHERE id = 1;

-- 11. JSON_REPLACE() - Replace existing values
UPDATE products 
SET attributes = JSON_REPLACE(attributes, '$.specs.ram', '32GB')
WHERE JSON_UNQUOTE(JSON_EXTRACT(attributes, '$.model')) = 'XPS 13';

-- 12. JSON_REMOVE() - Remove values
UPDATE products 
SET metadata = JSON_REMOVE(metadata, '$.inventory')
WHERE id = 3;

-- 13. Complex JSON queries with aggregation
SELECT 
    JSON_UNQUOTE(JSON_EXTRACT(attributes, '$.brand')) as brand,
    COUNT(*) as product_count,
    AVG(price) as avg_price,
    JSON_ARRAYAGG(
        JSON_OBJECT(
            'name', name,
            'price', price,
            'rating', JSON_UNQUOTE(JSON_EXTRACT(metadata, '$.ratings.average'))
        )
    ) as products
FROM products
GROUP BY JSON_UNQUOTE(JSON_EXTRACT(attributes, '$.brand'));

-- 14. JSON path expressions with filtering
SELECT 
    name,
    price,
    JSON_EXTRACT(attributes, '$.specs') as specifications,
    JSON_EXTRACT(metadata, '$.ratings') as ratings
FROM products
WHERE JSON_EXTRACT(metadata, '$.ratings.average') > 4.3;

-- 15. JSON validation and error handling
SELECT 
    name,
    JSON_VALID(attributes) as is_valid_json,
    JSON_TYPE(attributes) as json_type,
    JSON_PRETTY(attributes) as formatted_json
FROM products;
```

### Q84. What is MySQL Partitioning and when to use it?
**Answer:** Partitioning divides a table into smaller, more manageable pieces while maintaining the logical structure of the table.

**Real-time Example:**
```sql
-- 1. Range Partitioning by date
CREATE TABLE sales_partitioned (
    id INT AUTO_INCREMENT,
    sale_date DATE,
    customer_id INT,
    product_id INT,
    amount DECIMAL(10,2),
    region VARCHAR(50),
    PRIMARY KEY (id, sale_date)
) PARTITION BY RANGE (YEAR(sale_date)) (
    PARTITION p2023 VALUES LESS THAN (2024),
    PARTITION p2024 VALUES LESS THAN (2025),
    PARTITION p2025 VALUES LESS THAN (2026),
    PARTITION p_future VALUES LESS THAN MAXVALUE
);

-- Insert sample data
INSERT INTO sales_partitioned (sale_date, customer_id, product_id, amount, region) VALUES
('2023-12-15', 101, 1, 100.00, 'North'),
('2024-01-15', 102, 2, 150.00, 'South'),
('2024-06-15', 103, 1, 200.00, 'East'),
('2025-01-15', 104, 3, 250.00, 'West');

-- 2. List Partitioning by region
CREATE TABLE customers_partitioned (
    id INT AUTO_INCREMENT,
    name VARCHAR(100),
    email VARCHAR(100),
    region VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (id, region)
) PARTITION BY LIST COLUMNS(region) (
    PARTITION p_north VALUES IN ('North', 'Northeast'),
    PARTITION p_south VALUES IN ('South', 'Southeast'),
    PARTITION p_east VALUES IN ('East', 'Northeast'),
    PARTITION p_west VALUES IN ('West', 'Northwest'),
    PARTITION p_other VALUES IN ('Central', 'International')
);

-- 3. Hash Partitioning for even distribution
CREATE TABLE logs_partitioned (
    id INT AUTO_INCREMENT,
    log_level ENUM('DEBUG', 'INFO', 'WARNING', 'ERROR'),
    message TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (id)
) PARTITION BY HASH(id) PARTITIONS 4;

-- 4. Composite Partitioning (Range + Hash)
CREATE TABLE orders_partitioned (
    id INT AUTO_INCREMENT,
    order_date DATE,
    customer_id INT,
    total_amount DECIMAL(10,2),
    status ENUM('pending', 'processing', 'shipped', 'delivered'),
    PRIMARY KEY (id, order_date, customer_id)
) PARTITION BY RANGE (YEAR(order_date))
SUBPARTITION BY HASH(customer_id) SUBPARTITIONS 2 (
    PARTITION p2023 VALUES LESS THAN (2024),
    PARTITION p2024 VALUES LESS THAN (2025),
    PARTITION p2025 VALUES LESS THAN (2026)
);

-- 5. Partition management operations
-- Add new partition
ALTER TABLE sales_partitioned ADD PARTITION (
    PARTITION p2026 VALUES LESS THAN (2027)
);

-- Drop old partition
ALTER TABLE sales_partitioned DROP PARTITION p2023;

-- Truncate partition
ALTER TABLE sales_partitioned TRUNCATE PARTITION p2024;

-- 6. Query specific partitions
SELECT * FROM sales_partitioned PARTITION (p2024);
SELECT * FROM sales_partitioned PARTITION (p2025);

-- 7. Partition pruning demonstration
EXPLAIN SELECT * FROM sales_partitioned 
WHERE sale_date BETWEEN '2024-01-01' AND '2024-12-31';

-- 8. Information about partitions
SELECT 
    TABLE_NAME,
    PARTITION_NAME,
    PARTITION_ORDINAL_POSITION,
    PARTITION_METHOD,
    PARTITION_EXPRESSION,
    PARTITION_DESCRIPTION,
    TABLE_ROWS
FROM information_schema.PARTITIONS
WHERE TABLE_SCHEMA = 'your_database' 
AND TABLE_NAME = 'sales_partitioned';

-- 9. Maintenance operations
-- Analyze partition
ALTER TABLE sales_partitioned ANALYZE PARTITION p2024;

-- Check partition
ALTER TABLE sales_partitioned CHECK PARTITION p2024;

-- Repair partition
ALTER TABLE sales_partitioned REPAIR PARTITION p2024;

-- Optimize partition
ALTER TABLE sales_partitioned OPTIMIZE PARTITION p2024;

-- 10. Best practices for partitioning
-- Use partitioning for:
-- - Large tables (> millions of rows)
-- - Time-series data
-- - Data with clear partitioning criteria
-- - Tables that need to drop old data regularly

-- Example: Log table with automatic partition management
CREATE TABLE application_logs (
    id BIGINT AUTO_INCREMENT,
    log_date DATETIME,
    level ENUM('DEBUG', 'INFO', 'WARNING', 'ERROR'),
    message TEXT,
    user_id INT,
    PRIMARY KEY (id, log_date)
) PARTITION BY RANGE (TO_DAYS(log_date)) (
    PARTITION p_old VALUES LESS THAN (TO_DAYS('2024-01-01')),
    PARTITION p_2024_01 VALUES LESS THAN (TO_DAYS('2024-02-01')),
    PARTITION p_2024_02 VALUES LESS THAN (TO_DAYS('2024-03-01')),
    PARTITION p_2024_03 VALUES LESS THAN (TO_DAYS('2024-04-01')),
    PARTITION p_future VALUES LESS THAN MAXVALUE
);

-- Stored procedure for automatic partition management
DELIMITER //
CREATE PROCEDURE CreateMonthlyPartition(IN table_name VARCHAR(64), IN partition_date DATE)
BEGIN
    DECLARE partition_name VARCHAR(64);
    DECLARE next_month DATE;
    DECLARE sql_stmt TEXT;
    
    SET partition_name = CONCAT('p_', DATE_FORMAT(partition_date, '%Y_%m'));
    SET next_month = DATE_ADD(LAST_DAY(partition_date), INTERVAL 1 DAY);
    
    SET sql_stmt = CONCAT(
        'ALTER TABLE ', table_name, 
        ' ADD PARTITION (PARTITION ', partition_name, 
        ' VALUES LESS THAN (TO_DAYS(''', next_month, ''')))'
    );
    
    SET @sql = sql_stmt;
    PREPARE stmt FROM @sql;
    EXECUTE stmt;
    DEALLOCATE PREPARE stmt;
END //
DELIMITER ;

-- Usage
CALL CreateMonthlyPartition('application_logs', '2024-04-01');
```

### Q85. Explain MySQL Replication and its types
**Answer:** MySQL replication allows you to copy data from one MySQL database server (master) to one or more MySQL database servers (slaves).

**Real-time Example:**
```sql
-- Master server configuration (my.cnf)
[mysqld]
server-id = 1
log-bin = mysql-bin
binlog-format = ROW
sync_binlog = 1
innodb_flush_log_at_trx_commit = 1

-- Slave server configuration (my.cnf)
[mysqld]
server-id = 2
relay-log = mysql-relay-bin
read-only = 1
log-slave-updates = 1

-- 1. Set up replication user on master
CREATE USER 'repl_user'@'%' IDENTIFIED BY 'secure_password';
GRANT REPLICATION SLAVE ON *.* TO 'repl_user'@'%';
FLUSH PRIVILEGES;

-- 2. Get master status
SHOW MASTER STATUS;
-- Note: File: mysql-bin.000001, Position: 154

-- 3. Configure slave
CHANGE MASTER TO
    MASTER_HOST = '192.168.1.100',
    MASTER_USER = 'repl_user',
    MASTER_PASSWORD = 'secure_password',
    MASTER_LOG_FILE = 'mysql-bin.000001',
    MASTER_LOG_POS = 154;

-- 4. Start slave
START SLAVE;

-- 5. Check slave status
SHOW SLAVE STATUS\G

-- 6. Monitor replication
SELECT 
    MASTER_HOST,
    MASTER_USER,
    MASTER_PORT,
    MASTER_LOG_FILE,
    READ_MASTER_LOG_POS,
    SLAVE_IO_RUNNING,
    SLAVE_SQL_RUNNING,
    SECONDS_BEHIND_MASTER
FROM information_schema.SLAVE_STATUS;

-- 7. Replication filtering
-- On master: Filter databases
[mysqld]
binlog-do-db = important_db
binlog-ignore-db = test_db

-- On slave: Filter tables
CHANGE REPLICATION FILTER REPLICATE_DO_TABLE = ('important_db.important_table');

-- 8. Multi-source replication (MySQL 5.7+)
-- Configure multiple masters
CHANGE MASTER TO
    MASTER_HOST = '192.168.1.100',
    MASTER_USER = 'repl_user',
    MASTER_PASSWORD = 'secure_password',
    MASTER_LOG_FILE = 'mysql-bin.000001',
    MASTER_LOG_POS = 154
FOR CHANNEL 'master1';

CHANGE MASTER TO
    MASTER_HOST = '192.168.1.101',
    MASTER_USER = 'repl_user',
    MASTER_PASSWORD = 'secure_password',
    MASTER_LOG_FILE = 'mysql-bin.000001',
    MASTER_LOG_POS = 154
FOR CHANNEL 'master2';

-- Start specific channel
START SLAVE FOR CHANNEL 'master1';

-- 9. GTID (Global Transaction Identifier) replication
-- Master configuration
[mysqld]
gtid-mode = ON
enforce-gtid-consistency = ON

-- Slave configuration with GTID
CHANGE MASTER TO
    MASTER_HOST = '192.168.1.100',
    MASTER_USER = 'repl_user',
    MASTER_PASSWORD = 'secure_password',
    MASTER_AUTO_POSITION = 1;

-- 10. Replication monitoring queries
-- Check replication lag
SELECT 
    MASTER_HOST,
    SECONDS_BEHIND_MASTER,
    SLAVE_IO_RUNNING,
    SLAVE_SQL_RUNNING,
    LAST_IO_ERROR,
    LAST_SQL_ERROR
FROM information_schema.SLAVE_STATUS;

-- Check binary log events
SHOW BINLOG EVENTS IN 'mysql-bin.000001' FROM 4 LIMIT 10;

-- Check replication threads
SHOW PROCESSLIST;

-- 11. Replication troubleshooting
-- Skip error on slave
SET GLOBAL sql_slave_skip_counter = 1;
START SLAVE;

-- Reset slave
STOP SLAVE;
RESET SLAVE;
-- Reconfigure and start again

-- 12. Backup and restore for replication
-- On master: Create backup
mysqldump --all-databases --master-data=2 --single-transaction > backup.sql

-- On slave: Restore and configure
mysql < backup.sql
-- Then configure replication as shown above

-- 13. Replication performance tuning
-- Master optimizations
[mysqld]
binlog-cache-size = 1M
max-binlog-cache-size = 2G
sync-binlog = 1

-- Slave optimizations
[mysqld]
slave-net-timeout = 60
slave-sql-threads = 4  # Parallel replication
slave-parallel-workers = 4
```

### Q86. What are MySQL Stored Procedures and Functions?
**Answer:** Stored procedures and functions are precompiled SQL statements stored in the database that can be called with parameters.

**Real-time Example:**
```sql
-- 1. Simple stored procedure
DELIMITER //
CREATE PROCEDURE GetCustomerOrders(IN customer_id INT)
BEGIN
    SELECT 
        o.id,
        o.order_date,
        o.total_amount,
        o.status
    FROM orders o
    WHERE o.customer_id = customer_id
    ORDER BY o.order_date DESC;
END //
DELIMITER ;

-- Usage
CALL GetCustomerOrders(101);

-- 2. Stored procedure with multiple parameters
DELIMITER //
CREATE PROCEDURE CreateOrder(
    IN p_customer_id INT,
    IN p_total_amount DECIMAL(10,2),
    IN p_status VARCHAR(20),
    OUT p_order_id INT
)
BEGIN
    DECLARE EXIT HANDLER FOR SQLEXCEPTION
    BEGIN
        ROLLBACK;
        RESIGNAL;
    END;
    
    START TRANSACTION;
    
    INSERT INTO orders (customer_id, total_amount, status, order_date)
    VALUES (p_customer_id, p_total_amount, p_status, NOW());
    
    SET p_order_id = LAST_INSERT_ID();
    
    COMMIT;
END //
DELIMITER ;

-- Usage
CALL CreateOrder(101, 299.99, 'pending', @new_order_id);
SELECT @new_order_id;

-- 3. Stored function
DELIMITER //
CREATE FUNCTION CalculateDiscount(amount DECIMAL(10,2), customer_type VARCHAR(20))
RETURNS DECIMAL(10,2)
READS SQL DATA
DETERMINISTIC
BEGIN
    DECLARE discount DECIMAL(10,2) DEFAULT 0;
    
    CASE customer_type
        WHEN 'VIP' THEN SET discount = amount * 0.15;
        WHEN 'Premium' THEN SET discount = amount * 0.10;
        WHEN 'Regular' THEN SET discount = amount * 0.05;
        ELSE SET discount = 0;
    END CASE;
    
    RETURN discount;
END //
DELIMITER ;

-- Usage
SELECT 
    id,
    total_amount,
    customer_type,
    CalculateDiscount(total_amount, customer_type) as discount,
    total_amount - CalculateDiscount(total_amount, customer_type) as final_amount
FROM orders;

-- 4. Complex stored procedure with cursors
DELIMITER //
CREATE PROCEDURE ProcessBulkOrders(IN order_ids TEXT)
BEGIN
    DECLARE done INT DEFAULT FALSE;
    DECLARE order_id INT;
    DECLARE order_cursor CURSOR FOR 
        SELECT CAST(SUBSTRING_INDEX(SUBSTRING_INDEX(order_ids, ',', numbers.n), ',', -1) AS UNSIGNED) as id
        FROM (SELECT 1 n UNION SELECT 2 UNION SELECT 3 UNION SELECT 4 UNION SELECT 5) numbers
        WHERE CHAR_LENGTH(order_ids) - CHAR_LENGTH(REPLACE(order_ids, ',', '')) >= numbers.n - 1;
    DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;
    
    DECLARE EXIT HANDLER FOR SQLEXCEPTION
    BEGIN
        ROLLBACK;
        RESIGNAL;
    END;
    
    START TRANSACTION;
    
    OPEN order_cursor;
    
    read_loop: LOOP
        FETCH order_cursor INTO order_id;
        IF done THEN
            LEAVE read_loop;
        END IF;
        
        -- Process each order
        UPDATE orders 
        SET status = 'processing', 
            updated_at = NOW() 
        WHERE id = order_id;
        
        -- Log the processing
        INSERT INTO order_logs (order_id, action, created_at)
        VALUES (order_id, 'Bulk processing started', NOW());
        
    END LOOP;
    
    CLOSE order_cursor;
    COMMIT;
END //
DELIMITER ;

-- Usage
CALL ProcessBulkOrders('1,2,3,4,5');

-- 5. Stored procedure with error handling
DELIMITER //
CREATE PROCEDURE SafeOrderUpdate(
    IN p_order_id INT,
    IN p_new_status VARCHAR(20),
    OUT p_success BOOLEAN,
    OUT p_message VARCHAR(255)
)
BEGIN
    DECLARE order_exists INT DEFAULT 0;
    DECLARE current_status VARCHAR(20);
    DECLARE EXIT HANDLER FOR SQLEXCEPTION
    BEGIN
        SET p_success = FALSE;
        SET p_message = 'Database error occurred';
        ROLLBACK;
    END;
    
    START TRANSACTION;
    
    -- Check if order exists
    SELECT COUNT(*) INTO order_exists FROM orders WHERE id = p_order_id;
    
    IF order_exists = 0 THEN
        SET p_success = FALSE;
        SET p_message = 'Order not found';
        ROLLBACK;
    ELSE
        -- Get current status
        SELECT status INTO current_status FROM orders WHERE id = p_order_id;
        
        -- Validate status transition
        IF current_status = 'delivered' AND p_new_status != 'delivered' THEN
            SET p_success = FALSE;
            SET p_message = 'Cannot change status of delivered order';
            ROLLBACK;
        ELSE
            -- Update order
            UPDATE orders 
            SET status = p_new_status, updated_at = NOW() 
            WHERE id = p_order_id;
            
            -- Log the change
            INSERT INTO order_logs (order_id, action, created_at)
            VALUES (p_order_id, CONCAT('Status changed to ', p_new_status), NOW());
            
            SET p_success = TRUE;
            SET p_message = 'Order updated successfully';
            COMMIT;
        END IF;
    END IF;
END //
DELIMITER ;

-- Usage
CALL SafeOrderUpdate(1, 'shipped', @success, @message);
SELECT @success, @message;

-- 6. Recursive stored procedure
DELIMITER //
CREATE PROCEDURE GetCategoryHierarchy(IN root_category_id INT)
BEGIN
    DECLARE done INT DEFAULT FALSE;
    DECLARE cat_id INT;
    DECLARE cat_name VARCHAR(100);
    DECLARE parent_id INT;
    DECLARE level INT DEFAULT 0;
    
    -- Create temporary table for results
    DROP TEMPORARY TABLE IF EXISTS temp_categories;
    CREATE TEMPORARY TABLE temp_categories (
        id INT,
        name VARCHAR(100),
        parent_id INT,
        level INT
    );
    
    -- Insert root category
    INSERT INTO temp_categories (id, name, parent_id, level)
    SELECT id, name, parent_id, 0 FROM categories WHERE id = root_category_id;
    
    -- Recursive loop
    WHILE ROW_COUNT() > 0 DO
        SET level = level + 1;
        
        INSERT INTO temp_categories (id, name, parent_id, level)
        SELECT c.id, c.name, c.parent_id, level
        FROM categories c
        INNER JOIN temp_categories tc ON c.parent_id = tc.id
        WHERE tc.level = level - 1;
        
    END WHILE;
    
    -- Return results
    SELECT * FROM temp_categories ORDER BY level, name;
    
    DROP TEMPORARY TABLE temp_categories;
END //
DELIMITER ;

-- 7. Stored procedure for data migration
DELIMITER //
CREATE PROCEDURE MigrateOldOrders()
BEGIN
    DECLARE done INT DEFAULT FALSE;
    DECLARE old_order_id INT;
    DECLARE customer_id INT;
    DECLARE order_date DATE;
    DECLARE total_amount DECIMAL(10,2);
    
    DECLARE old_orders_cursor CURSOR FOR 
        SELECT id, customer_id, order_date, total_amount 
        FROM old_orders 
        WHERE migrated = FALSE;
    
    DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;
    
    DECLARE EXIT HANDLER FOR SQLEXCEPTION
    BEGIN
        ROLLBACK;
        RESIGNAL;
    END;
    
    START TRANSACTION;
    
    OPEN old_orders_cursor;
    
    read_loop: LOOP
        FETCH old_orders_cursor INTO old_order_id, customer_id, order_date, total_amount;
        IF done THEN
            LEAVE read_loop;
        END IF;
        
        -- Migrate order
        INSERT INTO orders (customer_id, order_date, total_amount, status, created_at)
        VALUES (customer_id, order_date, total_amount, 'migrated', NOW());
        
        -- Mark as migrated
        UPDATE old_orders SET migrated = TRUE WHERE id = old_order_id;
        
    END LOOP;
    
    CLOSE old_orders_cursor;
    COMMIT;
    
    SELECT 'Migration completed successfully' as result;
END //
DELIMITER ;
```

---

## MySQL Triggers and Events

### Q86b. What are MySQL Triggers and when to use them?
**Answer:** Triggers are stored programs that automatically execute when a specific event occurs in a table.

**Real-time Example:**
```sql
-- 1. Create audit log table
CREATE TABLE user_audit_log (
    log_id INT AUTO_INCREMENT PRIMARY KEY,
    user_id INT,
    action_type VARCHAR(50),
    old_data JSON,
    new_data JSON,
    changed_by VARCHAR(100),
    changed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_user_id (user_id),
    INDEX idx_changed_at (changed_at)
) ENGINE=InnoDB;

-- 2. BEFORE INSERT Trigger - Validate data before insertion
DELIMITER //
CREATE TRIGGER users_before_insert
BEFORE INSERT ON users
FOR EACH ROW
BEGIN
    -- Validate email format
    IF NEW.email NOT REGEXP '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$' THEN
        SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Invalid email format';
    END IF;
    
    -- Set default values
    IF NEW.is_active IS NULL THEN
        SET NEW.is_active = TRUE;
    END IF;
    
    -- Auto-generate username if not provided
    IF NEW.username IS NULL OR NEW.username = '' THEN
        SET NEW.username = CONCAT('user_', NEW.user_id);
    END IF;
END //
DELIMITER ;

-- 3. AFTER INSERT Trigger - Audit trail
DELIMITER //
CREATE TRIGGER users_after_insert
AFTER INSERT ON users
FOR EACH ROW
BEGIN
    INSERT INTO user_audit_log (user_id, action_type, new_data, changed_by)
    VALUES (
        NEW.user_id,
        'INSERT',
        JSON_OBJECT(
            'email', NEW.email,
            'username', NEW.username,
            'first_name', NEW.first_name,
            'last_name', NEW.last_name
        ),
        USER()
    );
END //
DELIMITER ;

-- 4. BEFORE UPDATE Trigger - Log changes
DELIMITER //
CREATE TRIGGER users_before_update
BEFORE UPDATE ON users
FOR EACH ROW
BEGIN
    -- Log changes if email or username changed
    IF OLD.email != NEW.email OR OLD.username != NEW.username THEN
        INSERT INTO user_audit_log (user_id, action_type, old_data, new_data, changed_by)
        VALUES (
            NEW.user_id,
            'UPDATE',
            JSON_OBJECT('email', OLD.email, 'username', OLD.username),
            JSON_OBJECT('email', NEW.email, 'username', NEW.username),
            USER()
        );
    END IF;
    
    -- Auto-update updated_at timestamp
    SET NEW.updated_at = NOW();
END //
DELIMITER ;

-- 5. AFTER DELETE Trigger - Archive deleted records
CREATE TABLE users_deleted (
    user_id INT,
    username VARCHAR(50),
    email VARCHAR(100),
    deleted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    deleted_by VARCHAR(100)
) ENGINE=InnoDB;

DELIMITER //
CREATE TRIGGER users_after_delete
AFTER DELETE ON users
FOR EACH ROW
BEGIN
    INSERT INTO users_deleted (user_id, username, email, deleted_by)
    VALUES (OLD.user_id, OLD.username, OLD.email, USER());
END //
DELIMITER ;

-- 6. Stock Management with Triggers
CREATE TABLE inventory_movements (
    movement_id INT AUTO_INCREMENT PRIMARY KEY,
    product_id INT,
    movement_type ENUM('in', 'out', 'adjustment'),
    quantity INT NOT NULL,
    reason VARCHAR(200),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
) ENGINE=InnoDB;

DELIMITER //
CREATE TRIGGER orders_after_insert_stock_update
AFTER INSERT ON order_items
FOR EACH ROW
BEGIN
    UPDATE products
    SET stock_quantity = stock_quantity - NEW.quantity
    WHERE product_id = NEW.product_id;
    
    INSERT INTO inventory_movements (
        product_id, movement_type, quantity, reason
    ) VALUES (
        NEW.product_id, 'out', NEW.quantity, CONCAT('Order: ', NEW.order_id)
    );
END //
DELIMITER ;

-- 7. View all triggers
SHOW TRIGGERS;
SHOW TRIGGERS LIKE 'users%';
SHOW CREATE TRIGGER users_before_insert;

-- 8. Drop trigger
DROP TRIGGER IF EXISTS users_before_insert;

-- 9. Event Scheduler (Scheduled Tasks)
-- Enable event scheduler
SET GLOBAL event_scheduler = ON;
SHOW VARIABLES LIKE 'event_scheduler';

-- Create event to archive old records
DELIMITER //
CREATE EVENT archive_old_audit_logs
ON SCHEDULE EVERY 1 DAY
STARTS CURRENT_TIMESTAMP
DO
BEGIN
    INSERT INTO user_audit_log_archive
    SELECT * FROM user_audit_log
    WHERE changed_at < DATE_SUB(NOW(), INTERVAL 90 DAY);
    
    DELETE FROM user_audit_log
    WHERE changed_at < DATE_SUB(NOW(), INTERVAL 90 DAY);
END //
DELIMITER ;

-- 10. Event to update product statistics daily
CREATE EVENT update_daily_product_stats
ON SCHEDULE EVERY 1 DAY
STARTS TIMESTAMP(CURDATE(), '01:00:00')
DO
BEGIN
    UPDATE products p
    SET p.total_sold = (
        SELECT COALESCE(SUM(oi.quantity), 0)
        FROM order_items oi
        JOIN orders o ON oi.order_id = o.order_id
        WHERE oi.product_id = p.product_id AND o.status = 'delivered'
    );
END;

-- 11. View all events
SHOW EVENTS;
SHOW EVENTS LIKE 'archive%';
SHOW CREATE EVENT archive_old_audit_logs;

-- 12. Manage events
ALTER EVENT archive_old_audit_logs DISABLE;
ALTER EVENT archive_old_audit_logs ENABLE;
DROP EVENT IF EXISTS archive_old_audit_logs;
```

---

## MySQL Replication

### Q87. MySQL Master-Slave Replication Setup
**Answer:** MySQL replication allows you to maintain multiple copies of your database for backup, scaling, and high availability.

**Real-time Example:**
```sql
-- Master Server Configuration (my.cnf)
[mysqld]
server-id = 1
log-bin = mysql-bin
binlog-format = ROW
sync_binlog = 1
innodb_flush_log_at_trx_commit = 1

-- Create replication user on master
CREATE USER 'repl_user'@'%' IDENTIFIED BY 'secure_password';
GRANT REPLICATION SLAVE ON *.* TO 'repl_user'@'%';
FLUSH PRIVILEGES;

-- Show master status
SHOW MASTER STATUS;
-- Note: File: mysql-bin.000001, Position: 154

-- Slave Server Configuration (my.cnf)
[mysqld]
server-id = 2
relay-log = mysql-relay-bin
read_only = 1

-- Configure slave to connect to master
CHANGE MASTER TO
    MASTER_HOST='192.168.1.100',
    MASTER_USER='repl_user',
    MASTER_PASSWORD='secure_password',
    MASTER_LOG_FILE='mysql-bin.000001',
    MASTER_LOG_POS=154;

-- Start slave
START SLAVE;

-- Check slave status
SHOW SLAVE STATUS\G

-- Monitor replication lag
SELECT 
    MASTER_LOG_FILE,
    MASTER_LOG_POS,
    RELAY_LOG_FILE,
    RELAY_LOG_POS,
    SLAVE_IO_RUNNING,
    SLAVE_SQL_RUNNING,
    SECONDS_BEHIND_MASTER
FROM performance_schema.replication_applier_status_by_worker;
```

### Q88. MySQL Multi-Source Replication
**Answer:** Multi-source replication allows a single slave to replicate from multiple masters.

**Real-time Example:**
```sql
-- Configure multiple replication channels
-- Channel 1: Replicate from master1
CHANGE MASTER TO
    MASTER_HOST='master1.example.com',
    MASTER_USER='repl_user',
    MASTER_PASSWORD='password1',
    MASTER_LOG_FILE='mysql-bin.000001',
    MASTER_LOG_POS=154
FOR CHANNEL 'master1';

-- Channel 2: Replicate from master2
CHANGE MASTER TO
    MASTER_HOST='master2.example.com',
    MASTER_USER='repl_user',
    MASTER_PASSWORD='password2',
    MASTER_LOG_FILE='mysql-bin.000001',
    MASTER_LOG_POS=154
FOR CHANNEL 'master2';

-- Start both channels
START SLAVE FOR CHANNEL 'master1';
START SLAVE FOR CHANNEL 'master2';

-- Check status for specific channel
SHOW SLAVE STATUS FOR CHANNEL 'master1'\G

-- Stop specific channel
STOP SLAVE FOR CHANNEL 'master1';

-- Replication filtering
-- Replicate only specific databases
CHANGE REPLICATION FILTER REPLICATE_DO_DB = (ecommerce, analytics);

-- Replicate only specific tables
CHANGE REPLICATION FILTER REPLICATE_DO_TABLE = (ecommerce.orders, ecommerce.customers);
```

### Q89. MySQL GTID (Global Transaction Identifier)
**Answer:** GTID provides a unique identifier for each transaction, making replication more reliable and easier to manage.

**Real-time Example:**
```sql
-- Enable GTID on master (my.cnf)
[mysqld]
gtid-mode = ON
enforce-gtid-consistency = ON
log-slave-updates = ON

-- Enable GTID on slave (my.cnf)
[mysqld]
gtid-mode = ON
enforce-gtid-consistency = ON
log-slave-updates = ON

-- Configure slave with GTID
CHANGE MASTER TO
    MASTER_HOST='192.168.1.100',
    MASTER_USER='repl_user',
    MASTER_PASSWORD='secure_password',
    MASTER_AUTO_POSITION = 1;

-- Check GTID status
SELECT @@gtid_mode;
SELECT @@enforce_gtid_consistency;

-- View GTID executed
SHOW MASTER STATUS;
-- GTID: 1-1-1, 1-1-2, 1-1-3

-- View GTID purged
SELECT @@gtid_purged;

-- Skip transactions with GTID
SET GTID_NEXT = '1-1-4';
BEGIN;
COMMIT;
SET GTID_NEXT = 'AUTOMATIC';
```

---

## MySQL Partitioning

### Q90. MySQL Table Partitioning Strategies
**Answer:** Partitioning divides large tables into smaller, more manageable pieces for better performance and maintenance.

**Real-time Example:**
```sql
-- Range Partitioning by Date
CREATE TABLE sales (
    id INT AUTO_INCREMENT,
    sale_date DATE NOT NULL,
    customer_id INT,
    amount DECIMAL(10,2),
    PRIMARY KEY (id, sale_date)
) PARTITION BY RANGE (YEAR(sale_date)) (
    PARTITION p2020 VALUES LESS THAN (2021),
    PARTITION p2021 VALUES LESS THAN (2022),
    PARTITION p2022 VALUES LESS THAN (2023),
    PARTITION p2023 VALUES LESS THAN (2024),
    PARTITION p_future VALUES LESS THAN MAXVALUE
);

-- List Partitioning by Region
CREATE TABLE customers (
    id INT AUTO_INCREMENT,
    name VARCHAR(100),
    region VARCHAR(50),
    email VARCHAR(100),
    PRIMARY KEY (id, region)
) PARTITION BY LIST COLUMNS(region) (
    PARTITION p_north VALUES IN ('NY', 'MA', 'CT'),
    PARTITION p_south VALUES IN ('FL', 'GA', 'SC'),
    PARTITION p_west VALUES IN ('CA', 'WA', 'OR'),
    PARTITION p_other VALUES IN ('TX', 'IL', 'OH')
);

-- Hash Partitioning for Load Distribution
CREATE TABLE user_sessions (
    id INT AUTO_INCREMENT,
    user_id INT,
    session_data TEXT,
    created_at TIMESTAMP,
    PRIMARY KEY (id, user_id)
) PARTITION BY HASH(user_id) PARTITIONS 8;

-- Composite Partitioning (Range + Hash)
CREATE TABLE orders (
    id INT AUTO_INCREMENT,
    order_date DATE,
    customer_id INT,
    amount DECIMAL(10,2),
    PRIMARY KEY (id, order_date, customer_id)
) PARTITION BY RANGE (YEAR(order_date))
SUBPARTITION BY HASH(customer_id) SUBPARTITIONS 4 (
    PARTITION p2020 VALUES LESS THAN (2021),
    PARTITION p2021 VALUES LESS THAN (2022),
    PARTITION p2022 VALUES LESS THAN (2023)
);

-- Partition Management
-- Add new partition
ALTER TABLE sales ADD PARTITION (
    PARTITION p2024 VALUES LESS THAN (2025)
);

-- Drop old partition
ALTER TABLE sales DROP PARTITION p2020;

-- Truncate specific partition
ALTER TABLE sales TRUNCATE PARTITION p2021;

-- Query specific partition
SELECT * FROM sales PARTITION (p2022);

-- Check partition information
SELECT 
    TABLE_NAME,
    PARTITION_NAME,
    PARTITION_DESCRIPTION,
    TABLE_ROWS
FROM INFORMATION_SCHEMA.PARTITIONS 
WHERE TABLE_NAME = 'sales';
```

### Q91. MySQL Partition Pruning and Optimization
**Answer:** Partition pruning automatically eliminates partitions that don't match the query conditions, improving performance.

**Real-time Example:**
```sql
-- Automatic partition pruning
-- This query will only scan p2022 partition
SELECT * FROM sales 
WHERE sale_date BETWEEN '2022-01-01' AND '2022-12-31';

-- Check if partition pruning is working
EXPLAIN PARTITIONS SELECT * FROM sales 
WHERE sale_date BETWEEN '2022-01-01' AND '2022-12-31';

-- Manual partition selection
SELECT * FROM sales PARTITION (p2022, p2023)
WHERE sale_date BETWEEN '2022-01-01' AND '2023-12-31';

-- Partition-aware indexing
CREATE INDEX idx_sales_date_customer ON sales (sale_date, customer_id);

-- Analyze partition statistics
ANALYZE TABLE sales;

-- Optimize specific partition
OPTIMIZE TABLE sales PARTITION (p2022);

-- Check partition sizes
SELECT 
    PARTITION_NAME,
    TABLE_ROWS,
    DATA_LENGTH,
    INDEX_LENGTH,
    (DATA_LENGTH + INDEX_LENGTH) / 1024 / 1024 AS 'Size_MB'
FROM INFORMATION_SCHEMA.PARTITIONS 
WHERE TABLE_NAME = 'sales'
ORDER BY PARTITION_NAME;
```

---

## MySQL Backup & Recovery

### Q92. MySQL Backup Strategies
**Answer:** MySQL provides multiple backup methods including mysqldump, binary log backups, and physical backups.

**Real-time Example:**
```bash
# Full database backup
mysqldump --single-transaction --routines --triggers \
    --all-databases --master-data=2 > full_backup.sql

# Backup specific database
mysqldump --single-transaction --routines --triggers \
    --databases ecommerce > ecommerce_backup.sql

# Backup with compression
mysqldump --single-transaction --routines --triggers \
    --all-databases | gzip > backup_$(date +%Y%m%d_%H%M%S).sql.gz

# Incremental backup using binary logs
# First, enable binary logging
# my.cnf: log-bin = mysql-bin

# Full backup
mysqldump --single-transaction --master-data=2 \
    --all-databases > full_backup.sql

# Incremental backup (backup binary logs)
mysqlbinlog --start-datetime="2023-01-01 00:00:00" \
    --stop-datetime="2023-01-02 00:00:00" \
    mysql-bin.000001 mysql-bin.000002 > incremental_backup.sql

# Point-in-time recovery
# 1. Restore full backup
mysql < full_backup.sql

# 2. Apply incremental changes
mysqlbinlog --start-datetime="2023-01-01 00:00:00" \
    --stop-datetime="2023-01-01 12:00:00" \
    mysql-bin.000001 | mysql

# Physical backup using Percona XtraBackup
xtrabackup --backup --target-dir=/backup/full/
xtrabackup --prepare --target-dir=/backup/full/
xtrabackup --copy-back --target-dir=/backup/full/
```

### Q93. MySQL Recovery Procedures
**Answer:** Recovery procedures depend on the backup type and the point of failure.

**Real-time Example:**
```sql
-- Recovery from logical backup
-- 1. Stop MySQL service
systemctl stop mysql

-- 2. Remove existing data directory
rm -rf /var/lib/mysql/*

-- 3. Restore from backup
mysql < full_backup.sql

-- 4. Start MySQL service
systemctl start mysql

-- Recovery from physical backup
-- 1. Stop MySQL service
systemctl stop mysql

-- 2. Remove existing data directory
rm -rf /var/lib/mysql/*

-- 3. Copy backup files
cp -r /backup/full/* /var/lib/mysql/

-- 4. Set proper ownership
chown -R mysql:mysql /var/lib/mysql

-- 5. Start MySQL service
systemctl start mysql

-- Point-in-time recovery
-- 1. Restore full backup
mysql < full_backup.sql

-- 2. Apply binary logs up to specific time
mysqlbinlog --start-datetime="2023-01-01 00:00:00" \
    --stop-datetime="2023-01-01 12:00:00" \
    mysql-bin.000001 mysql-bin.000002 | mysql

-- Recovery from replication
-- 1. Stop slave
STOP SLAVE;

-- 2. Reset slave to start from beginning
RESET SLAVE;

-- 3. Configure new master position
CHANGE MASTER TO
    MASTER_LOG_FILE='mysql-bin.000001',
    MASTER_LOG_POS=154;

-- 4. Start slave
START SLAVE;

-- Verify recovery
SHOW SLAVE STATUS\G
```

---

## MySQL Monitoring & Logging

### Q94. MySQL Performance Monitoring
**Answer:** MySQL provides comprehensive monitoring through performance schema, information schema, and various status variables.

**Real-time Example:**
```sql
-- Monitor active connections
SELECT 
    ID,
    USER,
    HOST,
    DB,
    COMMAND,
    TIME,
    STATE,
    INFO
FROM INFORMATION_SCHEMA.PROCESSLIST
WHERE COMMAND != 'Sleep';

-- Monitor slow queries
SELECT 
    query_time,
    lock_time,
    rows_sent,
    rows_examined,
    sql_text
FROM mysql.slow_log
WHERE start_time > DATE_SUB(NOW(), INTERVAL 1 HOUR)
ORDER BY query_time DESC;

-- Monitor table locks
SELECT 
    OBJECT_SCHEMA,
    OBJECT_NAME,
    LOCK_TYPE,
    LOCK_DURATION,
    LOCK_STATUS
FROM performance_schema.metadata_locks
WHERE OBJECT_SCHEMA IS NOT NULL;

-- Monitor InnoDB status
SHOW ENGINE INNODB STATUS;

-- Monitor replication lag
SELECT 
    MASTER_LOG_FILE,
    MASTER_LOG_POS,
    RELAY_LOG_FILE,
    RELAY_LOG_POS,
    SLAVE_IO_RUNNING,
    SLAVE_SQL_RUNNING,
    SECONDS_BEHIND_MASTER
FROM performance_schema.replication_applier_status_by_worker;

-- Monitor buffer pool usage
SELECT 
    POOL_ID,
    POOL_SIZE,
    FREE_BUFFERS,
    DATABASE_PAGES,
    OLD_DATABASE_PAGES,
    MODIFIED_DATABASE_PAGES
FROM performance_schema.innodb_buffer_pool_stats;

-- Monitor query performance
SELECT 
    DIGEST_TEXT,
    COUNT_STAR,
    AVG_TIMER_WAIT/1000000000 as AVG_TIME_SEC,
    MAX_TIMER_WAIT/1000000000 as MAX_TIME_SEC,
    SUM_ROWS_EXAMINED,
    SUM_ROWS_SENT
FROM performance_schema.events_statements_summary_by_digest
ORDER BY AVG_TIMER_WAIT DESC
LIMIT 10;
```

### Q95. MySQL Logging Configuration
**Answer:** MySQL provides various logging options for debugging, auditing, and performance analysis.

**Real-time Example:**
```sql
-- Enable general query log
SET GLOBAL general_log = 'ON';
SET GLOBAL general_log_file = '/var/log/mysql/general.log';

-- Enable slow query log
SET GLOBAL slow_query_log = 'ON';
SET GLOBAL slow_query_log_file = '/var/log/mysql/slow.log';
SET GLOBAL long_query_time = 2; -- Log queries taking more than 2 seconds

-- Enable binary log
SET GLOBAL log_bin = 'ON';
SET GLOBAL binlog_format = 'ROW';

-- Enable error log (configured in my.cnf)
-- [mysqld]
-- log-error = /var/log/mysql/error.log

-- Check current logging settings
SHOW VARIABLES LIKE '%log%';

-- Monitor log file sizes
SELECT 
    'General Log' as Log_Type,
    @@general_log_file as Log_File,
    ROUND(@@general_log_file_size/1024/1024, 2) as Size_MB
UNION ALL
SELECT 
    'Slow Query Log',
    @@slow_query_log_file,
    ROUND(@@slow_query_log_file_size/1024/1024, 2)
UNION ALL
SELECT 
    'Binary Log',
    @@log_bin_basename,
    ROUND(@@log_bin_file_size/1024/1024, 2);

-- Rotate logs
FLUSH LOGS;

-- Purge old binary logs
PURGE BINARY LOGS TO 'mysql-bin.000010';
PURGE BINARY LOGS BEFORE '2023-01-01 00:00:00';

-- Monitor log rotation
SHOW BINARY LOGS;
```

---

## MySQL Performance Tuning

### Q96. MySQL Query Optimization
**Answer:** Query optimization involves analyzing execution plans, creating proper indexes, and rewriting queries for better performance.

**Real-time Example:**
```sql
-- Analyze query execution plan
EXPLAIN FORMAT=JSON
SELECT c.name, COUNT(o.id) as order_count, SUM(o.total) as total_spent
FROM customers c
JOIN orders o ON c.id = o.customer_id
WHERE c.created_at >= '2023-01-01'
GROUP BY c.id, c.name
HAVING COUNT(o.id) > 5
ORDER BY total_spent DESC;

-- Optimize with proper indexes
CREATE INDEX idx_customers_created ON customers(created_at);
CREATE INDEX idx_orders_customer_date ON orders(customer_id, order_date);
CREATE INDEX idx_orders_total ON orders(total);

-- Use covering indexes
CREATE INDEX idx_orders_covering ON orders(customer_id, order_date, total);

-- Optimize JOIN queries
-- Before: Nested loop join
SELECT c.name, o.order_date, o.total
FROM customers c
JOIN orders o ON c.id = o.customer_id
WHERE c.status = 'active';

-- After: Use appropriate JOIN type
SELECT c.name, o.order_date, o.total
FROM customers c
STRAIGHT_JOIN orders o ON c.id = o.customer_id
WHERE c.status = 'active';

-- Optimize subqueries
-- Before: Correlated subquery
SELECT c.name, 
    (SELECT COUNT(*) FROM orders o WHERE o.customer_id = c.id) as order_count
FROM customers c;

-- After: Use JOIN
SELECT c.name, COUNT(o.id) as order_count
FROM customers c
LEFT JOIN orders o ON c.id = o.customer_id
GROUP BY c.id, c.name;

-- Use LIMIT for pagination
SELECT * FROM orders 
WHERE customer_id = 123
ORDER BY order_date DESC
LIMIT 20 OFFSET 0;

-- Optimize with query hints
SELECT /*+ USE_INDEX(orders, idx_orders_customer_date) */
    c.name, o.order_date, o.total
FROM customers c
JOIN orders o ON c.id = o.customer_id
WHERE o.order_date >= '2023-01-01';
```

### Q97. MySQL Server Configuration Optimization
**Answer:** Server configuration optimization involves tuning various parameters for optimal performance.

**Real-time Example:**
```sql
-- InnoDB Configuration
-- my.cnf settings
[mysqld]
# Buffer pool size (70-80% of RAM)
innodb_buffer_pool_size = 2G

# Log file size
innodb_log_file_size = 256M
innodb_log_files_in_group = 2

# Flush method
innodb_flush_method = O_DIRECT

# Thread concurrency
innodb_thread_concurrency = 0

# I/O capacity
innodb_io_capacity = 2000
innodb_io_capacity_max = 4000

# Checkpoint age
innodb_max_dirty_pages_pct = 75

# Connection settings
max_connections = 200
max_connect_errors = 100000

# Query cache (MySQL 5.7 and earlier)
query_cache_type = 1
query_cache_size = 64M
query_cache_limit = 2M

# Temporary tables
tmp_table_size = 64M
max_heap_table_size = 64M

# Sort buffer
sort_buffer_size = 2M
read_buffer_size = 2M
read_rnd_buffer_size = 8M

# Thread settings
thread_cache_size = 8
table_open_cache = 4000

-- Monitor configuration effectiveness
SHOW STATUS LIKE 'Innodb_buffer_pool%';
SHOW STATUS LIKE 'Qcache%';
SHOW STATUS LIKE 'Threads%';
SHOW STATUS LIKE 'Connections%';

-- Check current settings
SHOW VARIABLES LIKE 'innodb_buffer_pool_size';
SHOW VARIABLES LIKE 'max_connections';
SHOW VARIABLES LIKE 'query_cache%';
```

---

## MySQL Security Hardening

### Q98. MySQL Security Configuration
**Answer:** MySQL security involves user management, encryption, and access control to protect data and prevent unauthorized access.

**Real-time Example:**
```sql
-- Create secure user accounts
CREATE USER 'app_user'@'localhost' IDENTIFIED BY 'strong_password_123!';
CREATE USER 'readonly_user'@'192.168.1.%' IDENTIFIED BY 'readonly_pass_456!';

-- Grant minimal required privileges
GRANT SELECT, INSERT, UPDATE, DELETE ON ecommerce.* TO 'app_user'@'localhost';
GRANT SELECT ON ecommerce.products TO 'readonly_user'@'192.168.1.%';

-- Remove anonymous users
DELETE FROM mysql.user WHERE User = '';
DELETE FROM mysql.user WHERE User = 'root' AND Host NOT IN ('localhost', '127.0.0.1', '::1');

-- Remove test database
DROP DATABASE IF EXISTS test;
DELETE FROM mysql.db WHERE Db = 'test' OR Db = 'test\\_%';

-- Enable SSL connections
-- Generate SSL certificates
-- mysql_ssl_rsa_setup

-- Configure SSL in my.cnf
[mysqld]
ssl-ca = /etc/mysql/ssl/ca.pem
ssl-cert = /etc/mysql/ssl/server-cert.pem
ssl-key = /etc/mysql/ssl/server-key.pem

-- Require SSL for specific users
ALTER USER 'app_user'@'localhost' REQUIRE SSL;

-- Enable password validation plugin
INSTALL PLUGIN validate_password SONAME 'validate_password.so';

-- Configure password policy
SET GLOBAL validate_password.policy = STRONG;
SET GLOBAL validate_password.length = 12;
SET GLOBAL validate_password.mixed_case_count = 2;
SET GLOBAL validate_password.number_count = 2;
SET GLOBAL validate_password.special_char_count = 2;

-- Enable audit logging
INSTALL PLUGIN audit_log SONAME 'audit_log.so';

-- Configure audit log
SET GLOBAL audit_log_policy = ALL;
SET GLOBAL audit_log_format = JSON;
SET GLOBAL audit_log_file = '/var/log/mysql/audit.log';

-- Monitor failed login attempts
SELECT 
    user,
    host,
    COUNT(*) as failed_attempts
FROM mysql.general_log
WHERE command_type = 'Connect'
    AND argument LIKE '%Access denied%'
    AND event_time > DATE_SUB(NOW(), INTERVAL 1 HOUR)
GROUP BY user, host
HAVING failed_attempts > 5;
```

### Q99. MySQL Data Encryption
**Answer:** MySQL provides encryption at rest and in transit to protect sensitive data.

**Real-time Example:**
```sql
-- Enable encryption at rest
-- Configure in my.cnf
[mysqld]
innodb_encrypt_tables = ON
innodb_encryption_threads = 4
innodb_encryption_rotate_key_age = 1

-- Encrypt specific tables
ALTER TABLE customers ENCRYPTION = 'Y';
ALTER TABLE orders ENCRYPTION = 'Y';
ALTER TABLE payments ENCRYPTION = 'Y';

-- Check encryption status
SELECT 
    SCHEMA_NAME,
    TABLE_NAME,
    CREATE_OPTIONS
FROM INFORMATION_SCHEMA.TABLES
WHERE CREATE_OPTIONS LIKE '%ENCRYPTION%';

-- Encrypt specific columns
CREATE TABLE sensitive_data (
    id INT PRIMARY KEY,
    name VARCHAR(100),
    ssn VARBINARY(255), -- Encrypted SSN
    credit_card VARBINARY(255) -- Encrypted credit card
);

-- Insert encrypted data
INSERT INTO sensitive_data (id, name, ssn, credit_card)
VALUES (
    1, 
    'John Doe',
    AES_ENCRYPT('123-45-6789', 'encryption_key'),
    AES_ENCRYPT('4111-1111-1111-1111', 'encryption_key')
);

-- Decrypt data
SELECT 
    id,
    name,
    AES_DECRYPT(ssn, 'encryption_key') as ssn,
    AES_DECRYPT(credit_card, 'encryption_key') as credit_card
FROM sensitive_data;

-- Enable binary log encryption
SET GLOBAL binlog_encryption = ON;

-- Enable redo log encryption
SET GLOBAL innodb_redo_log_encrypt = ON;

-- Enable undo log encryption
SET GLOBAL innodb_undo_log_encrypt = ON;

-- Check encryption status
SHOW STATUS LIKE 'Innodb_encryption%';
```

---

## MySQL JSON Functions

### Q100. Advanced MySQL JSON Operations
**Answer:** MySQL provides comprehensive JSON functions for storing, querying, and manipulating JSON data.

**Real-time Example:**
```sql
-- Create table with JSON column
CREATE TABLE products (
    id INT PRIMARY KEY AUTO_INCREMENT,
    name VARCHAR(100),
    specifications JSON,
    metadata JSON,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Insert JSON data
INSERT INTO products (name, specifications, metadata) VALUES
('Laptop', 
 '{"cpu": "Intel i7", "ram": "16GB", "storage": "512GB SSD", "screen": "15.6 inch"}',
 '{"brand": "Dell", "model": "XPS 15", "warranty": "2 years", "tags": ["gaming", "professional"]}'),
('Smartphone',
 '{"cpu": "Snapdragon 888", "ram": "8GB", "storage": "256GB", "camera": "108MP"}',
 '{"brand": "Samsung", "model": "Galaxy S21", "warranty": "1 year", "tags": ["flagship", "camera"]}');

-- Query JSON data
SELECT 
    name,
    JSON_EXTRACT(specifications, '$.cpu') as cpu,
    JSON_EXTRACT(specifications, '$.ram') as ram,
    JSON_EXTRACT(metadata, '$.brand') as brand
FROM products;

-- Use JSON path expressions
SELECT 
    name,
    specifications->>'$.cpu' as cpu,
    specifications->>'$.ram' as ram,
    metadata->>'$.brand' as brand,
    metadata->>'$.tags[0]' as primary_tag
FROM products;

-- Search in JSON arrays
SELECT name, specifications
FROM products
WHERE JSON_SEARCH(metadata, 'one', 'gaming') IS NOT NULL;

-- Filter by JSON values
SELECT name, specifications
FROM products
WHERE JSON_EXTRACT(specifications, '$.ram') = '16GB';

-- Update JSON data
UPDATE products 
SET specifications = JSON_SET(specifications, '$.ram', '32GB')
WHERE name = 'Laptop';

-- Add new JSON fields
UPDATE products 
SET metadata = JSON_INSERT(metadata, '$.price', 1299.99)
WHERE name = 'Laptop';

-- Remove JSON fields
UPDATE products 
SET metadata = JSON_REMOVE(metadata, '$.warranty')
WHERE name = 'Smartphone';

-- Aggregate JSON data
SELECT 
    JSON_EXTRACT(metadata, '$.brand') as brand,
    COUNT(*) as product_count,
    JSON_ARRAYAGG(name) as products
FROM products
GROUP BY JSON_EXTRACT(metadata, '$.brand');

-- Create JSON from relational data
SELECT 
    JSON_OBJECT(
        'id', id,
        'name', name,
        'specifications', specifications,
        'metadata', metadata
    ) as product_json
FROM products;

-- Validate JSON
SELECT 
    name,
    JSON_VALID(specifications) as is_valid_specs,
    JSON_VALID(metadata) as is_valid_metadata
FROM products;

-- Get JSON keys
SELECT 
    name,
    JSON_KEYS(specifications) as spec_keys,
    JSON_KEYS(metadata) as metadata_keys
FROM products;

-- Get JSON length
SELECT 
    name,
    JSON_LENGTH(specifications) as spec_length,
    JSON_LENGTH(metadata) as metadata_length
FROM products;
```

**Total MySQL Questions: 100+ with comprehensive explanations, real-world examples, and advanced concepts!**

---

## ðŸŽ¯ PRACTICAL INTERVIEW QUESTIONS & ANSWERS

### ðŸ”¥ Most Asked MySQL Interview Questions

#### **Q1. How would you design and implement a complete e-commerce database schema?**

**Answer:** Here's a comprehensive e-commerce database design:

```sql
-- 1. Users and Authentication
CREATE TABLE users (
    user_id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    username VARCHAR(50) NOT NULL UNIQUE,
    email VARCHAR(100) NOT NULL UNIQUE,
    password_hash VARCHAR(255) NOT NULL,
    first_name VARCHAR(50) NOT NULL,
    last_name VARCHAR(50) NOT NULL,
    phone VARCHAR(20),
    date_of_birth DATE,
    gender ENUM('male', 'female', 'other'),
    is_active BOOLEAN DEFAULT TRUE,
    email_verified BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    INDEX idx_email (email),
    INDEX idx_username (username),
    INDEX idx_created_at (created_at)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- 2. User Addresses
CREATE TABLE user_addresses (
    address_id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    user_id INT UNSIGNED NOT NULL,
    address_type ENUM('billing', 'shipping') NOT NULL,
    address_line1 VARCHAR(200) NOT NULL,
    address_line2 VARCHAR(200),
    city VARCHAR(100) NOT NULL,
    state VARCHAR(100) NOT NULL,
    postal_code VARCHAR(20) NOT NULL,
    country VARCHAR(100) NOT NULL,
    is_default BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    FOREIGN KEY (user_id) REFERENCES users(user_id) ON DELETE CASCADE,
    INDEX idx_user_id (user_id),
    INDEX idx_address_type (address_type)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- 3. Categories (Hierarchical)
CREATE TABLE categories (
    category_id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    parent_id INT UNSIGNED NULL,
    category_name VARCHAR(100) NOT NULL,
    slug VARCHAR(100) NOT NULL UNIQUE,
    description TEXT,
    image_url VARCHAR(500),
    sort_order INT DEFAULT 0,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    FOREIGN KEY (parent_id) REFERENCES categories(category_id) ON DELETE SET NULL,
    INDEX idx_parent_id (parent_id),
    INDEX idx_slug (slug),
    INDEX idx_is_active (is_active)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- 4. Products
CREATE TABLE products (
    product_id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    category_id INT UNSIGNED NOT NULL,
    product_name VARCHAR(200) NOT NULL,
    slug VARCHAR(200) NOT NULL UNIQUE,
    short_description TEXT,
    long_description LONGTEXT,
    sku VARCHAR(100) NOT NULL UNIQUE,
    price DECIMAL(10,2) NOT NULL,
    compare_price DECIMAL(10,2),
    cost_price DECIMAL(10,2),
    weight DECIMAL(8,2),
    dimensions JSON,
    stock_quantity INT DEFAULT 0,
    low_stock_threshold INT DEFAULT 5,
    track_inventory BOOLEAN DEFAULT TRUE,
    is_active BOOLEAN DEFAULT TRUE,
    is_featured BOOLEAN DEFAULT FALSE,
    meta_title VARCHAR(200),
    meta_description TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    FOREIGN KEY (category_id) REFERENCES categories(category_id) ON DELETE RESTRICT,
    INDEX idx_category_id (category_id),
    INDEX idx_sku (sku),
    INDEX idx_slug (slug),
    INDEX idx_price (price),
    INDEX idx_is_active (is_active),
    INDEX idx_is_featured (is_featured),
    FULLTEXT idx_search (product_name, short_description, long_description)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- 5. Product Images
CREATE TABLE product_images (
    image_id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    product_id INT UNSIGNED NOT NULL,
    image_url VARCHAR(500) NOT NULL,
    alt_text VARCHAR(200),
    sort_order INT DEFAULT 0,
    is_primary BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    FOREIGN KEY (product_id) REFERENCES products(product_id) ON DELETE CASCADE,
    INDEX idx_product_id (product_id),
    INDEX idx_is_primary (is_primary)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- 6. Product Variants (Size, Color, etc.)
CREATE TABLE product_variants (
    variant_id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    product_id INT UNSIGNED NOT NULL,
    variant_name VARCHAR(100) NOT NULL,
    sku VARCHAR(100) NOT NULL UNIQUE,
    price DECIMAL(10,2),
    stock_quantity INT DEFAULT 0,
    attributes JSON,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    FOREIGN KEY (product_id) REFERENCES products(product_id) ON DELETE CASCADE,
    INDEX idx_product_id (product_id),
    INDEX idx_sku (sku)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- 7. Shopping Cart
CREATE TABLE cart_items (
    cart_item_id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    user_id INT UNSIGNED NOT NULL,
    product_id INT UNSIGNED NOT NULL,
    variant_id INT UNSIGNED NULL,
    quantity INT NOT NULL DEFAULT 1,
    price DECIMAL(10,2) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    FOREIGN KEY (user_id) REFERENCES users(user_id) ON DELETE CASCADE,
    FOREIGN KEY (product_id) REFERENCES products(product_id) ON DELETE CASCADE,
    FOREIGN KEY (variant_id) REFERENCES product_variants(variant_id) ON DELETE CASCADE,
    UNIQUE KEY unique_user_product_variant (user_id, product_id, variant_id),
    INDEX idx_user_id (user_id)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- 8. Orders
CREATE TABLE orders (
    order_id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    user_id INT UNSIGNED NOT NULL,
    order_number VARCHAR(50) NOT NULL UNIQUE,
    status ENUM('pending', 'processing', 'shipped', 'delivered', 'cancelled', 'refunded') DEFAULT 'pending',
    payment_status ENUM('pending', 'paid', 'failed', 'refunded') DEFAULT 'pending',
    shipping_status ENUM('pending', 'shipped', 'delivered') DEFAULT 'pending',
    subtotal DECIMAL(10,2) NOT NULL,
    tax_amount DECIMAL(10,2) DEFAULT 0,
    shipping_amount DECIMAL(10,2) DEFAULT 0,
    discount_amount DECIMAL(10,2) DEFAULT 0,
    total_amount DECIMAL(10,2) NOT NULL,
    currency VARCHAR(3) DEFAULT 'USD',
    notes TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    FOREIGN KEY (user_id) REFERENCES users(user_id) ON DELETE RESTRICT,
    INDEX idx_user_id (user_id),
    INDEX idx_order_number (order_number),
    INDEX idx_status (status),
    INDEX idx_created_at (created_at)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- 9. Order Items
CREATE TABLE order_items (
    order_item_id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    order_id INT UNSIGNED NOT NULL,
    product_id INT UNSIGNED NOT NULL,
    variant_id INT UNSIGNED NULL,
    product_name VARCHAR(200) NOT NULL,
    product_sku VARCHAR(100) NOT NULL,
    quantity INT NOT NULL,
    unit_price DECIMAL(10,2) NOT NULL,
    total_price DECIMAL(10,2) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    FOREIGN KEY (order_id) REFERENCES orders(order_id) ON DELETE CASCADE,
    FOREIGN KEY (product_id) REFERENCES products(product_id) ON DELETE RESTRICT,
    FOREIGN KEY (variant_id) REFERENCES product_variants(variant_id) ON DELETE SET NULL,
    INDEX idx_order_id (order_id),
    INDEX idx_product_id (product_id)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- 10. Order Addresses
CREATE TABLE order_addresses (
    address_id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    order_id INT UNSIGNED NOT NULL,
    address_type ENUM('billing', 'shipping') NOT NULL,
    first_name VARCHAR(50) NOT NULL,
    last_name VARCHAR(50) NOT NULL,
    company VARCHAR(100),
    address_line1 VARCHAR(200) NOT NULL,
    address_line2 VARCHAR(200),
    city VARCHAR(100) NOT NULL,
    state VARCHAR(100) NOT NULL,
    postal_code VARCHAR(20) NOT NULL,
    country VARCHAR(100) NOT NULL,
    phone VARCHAR(20),
    
    FOREIGN KEY (order_id) REFERENCES orders(order_id) ON DELETE CASCADE,
    INDEX idx_order_id (order_id)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- 11. Payments
CREATE TABLE payments (
    payment_id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    order_id INT UNSIGNED NOT NULL,
    payment_method ENUM('credit_card', 'paypal', 'bank_transfer', 'cash_on_delivery') NOT NULL,
    payment_gateway VARCHAR(50),
    transaction_id VARCHAR(100),
    amount DECIMAL(10,2) NOT NULL,
    currency VARCHAR(3) DEFAULT 'USD',
    status ENUM('pending', 'completed', 'failed', 'refunded') DEFAULT 'pending',
    gateway_response JSON,
    processed_at TIMESTAMP NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    FOREIGN KEY (order_id) REFERENCES orders(order_id) ON DELETE CASCADE,
    INDEX idx_order_id (order_id),
    INDEX idx_transaction_id (transaction_id),
    INDEX idx_status (status)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- 12. Product Reviews
CREATE TABLE product_reviews (
    review_id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    product_id INT UNSIGNED NOT NULL,
    user_id INT UNSIGNED NOT NULL,
    order_id INT UNSIGNED NULL,
    rating INT NOT NULL CHECK (rating >= 1 AND rating <= 5),
    title VARCHAR(200),
    review_text TEXT,
    is_verified_purchase BOOLEAN DEFAULT FALSE,
    is_approved BOOLEAN DEFAULT FALSE,
    helpful_count INT DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    FOREIGN KEY (product_id) REFERENCES products(product_id) ON DELETE CASCADE,
    FOREIGN KEY (user_id) REFERENCES users(user_id) ON DELETE CASCADE,
    FOREIGN KEY (order_id) REFERENCES orders(order_id) ON DELETE SET NULL,
    UNIQUE KEY unique_user_product_review (user_id, product_id),
    INDEX idx_product_id (product_id),
    INDEX idx_rating (rating),
    INDEX idx_is_approved (is_approved)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- 13. Wishlist
CREATE TABLE wishlist (
    wishlist_id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    user_id INT UNSIGNED NOT NULL,
    product_id INT UNSIGNED NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    FOREIGN KEY (user_id) REFERENCES users(user_id) ON DELETE CASCADE,
    FOREIGN KEY (product_id) REFERENCES products(product_id) ON DELETE CASCADE,
    UNIQUE KEY unique_user_product_wishlist (user_id, product_id),
    INDEX idx_user_id (user_id)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- 14. Coupons and Discounts
CREATE TABLE coupons (
    coupon_id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    coupon_code VARCHAR(50) NOT NULL UNIQUE,
    description VARCHAR(200),
    discount_type ENUM('percentage', 'fixed_amount') NOT NULL,
    discount_value DECIMAL(10,2) NOT NULL,
    minimum_order_amount DECIMAL(10,2) DEFAULT 0,
    maximum_discount_amount DECIMAL(10,2),
    usage_limit INT,
    used_count INT DEFAULT 0,
    is_active BOOLEAN DEFAULT TRUE,
    valid_from TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    valid_until TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_coupon_code (coupon_code),
    INDEX idx_is_active (is_active),
    INDEX idx_valid_until (valid_until)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- 15. Analytics and Reporting Views
CREATE VIEW product_sales_summary AS
SELECT 
    p.product_id,
    p.product_name,
    p.sku,
    p.price,
    COUNT(oi.order_item_id) as total_orders,
    SUM(oi.quantity) as total_quantity_sold,
    SUM(oi.total_price) as total_revenue,
    AVG(pr.rating) as average_rating,
    COUNT(pr.review_id) as review_count
FROM products p
LEFT JOIN order_items oi ON p.product_id = oi.product_id
LEFT JOIN product_reviews pr ON p.product_id = pr.product_id AND pr.is_approved = TRUE
WHERE p.is_active = TRUE
GROUP BY p.product_id, p.product_name, p.sku, p.price;

-- 16. Sample Data Insertion
INSERT INTO categories (category_name, slug, description) VALUES
('Electronics', 'electronics', 'Electronic devices and gadgets'),
('Clothing', 'clothing', 'Fashion and apparel'),
('Books', 'books', 'Books and educational materials');

INSERT INTO products (category_id, product_name, slug, short_description, sku, price, stock_quantity) VALUES
(1, 'iPhone 15 Pro', 'iphone-15-pro', 'Latest iPhone with advanced features', 'IPH15PRO', 999.99, 50),
(2, 'Cotton T-Shirt', 'cotton-t-shirt', 'Comfortable cotton t-shirt', 'CTSHIRT', 29.99, 100),
(3, 'Programming Book', 'programming-book', 'Learn programming fundamentals', 'PROGBOOK', 49.99, 25);

-- 17. Useful Queries for E-commerce
-- Top selling products
SELECT 
    p.product_name,
    SUM(oi.quantity) as total_sold,
    SUM(oi.total_price) as total_revenue
FROM products p
JOIN order_items oi ON p.product_id = oi.product_id
JOIN orders o ON oi.order_id = o.order_id
WHERE o.status = 'delivered'
GROUP BY p.product_id, p.product_name
ORDER BY total_sold DESC
LIMIT 10;

-- Customer order history
SELECT 
    u.first_name,
    u.last_name,
    o.order_number,
    o.status,
    o.total_amount,
    o.created_at
FROM users u
JOIN orders o ON u.user_id = o.user_id
WHERE u.user_id = 1
ORDER BY o.created_at DESC;

-- Low stock products
SELECT 
    product_name,
    sku,
    stock_quantity,
    low_stock_threshold
FROM products
WHERE stock_quantity <= low_stock_threshold
AND is_active = TRUE;
```

#### **Q2. How would you optimize a slow-performing MySQL query?**

**Answer:** Here's a comprehensive approach to query optimization:

```sql
-- 1. Identify Slow Queries
-- Enable slow query log
SET GLOBAL slow_query_log = 'ON';
SET GLOBAL long_query_time = 2; -- Log queries taking more than 2 seconds
SET GLOBAL slow_query_log_file = '/var/log/mysql/slow.log';

-- 2. Analyze Query Performance
-- Use EXPLAIN to understand query execution plan
EXPLAIN FORMAT=JSON
SELECT 
    u.first_name,
    u.last_name,
    o.order_number,
    o.total_amount,
    p.product_name,
    oi.quantity
FROM users u
JOIN orders o ON u.user_id = o.user_id
JOIN order_items oi ON o.order_id = oi.order_id
JOIN products p ON oi.product_id = p.product_id
WHERE o.created_at >= '2024-01-01'
AND o.status = 'delivered'
ORDER BY o.created_at DESC;

-- 3. Optimize with Proper Indexing
-- Create composite indexes for common query patterns
CREATE INDEX idx_orders_status_date ON orders(status, created_at);
CREATE INDEX idx_order_items_product ON order_items(product_id, order_id);
CREATE INDEX idx_products_active ON products(is_active, product_id);

-- 4. Query Rewriting for Better Performance
-- Original slow query
SELECT * FROM orders 
WHERE user_id IN (
    SELECT user_id FROM users WHERE email LIKE '%@gmail.com'
)
AND status = 'delivered'
AND created_at >= '2024-01-01';

-- Optimized version with JOIN
SELECT o.* FROM orders o
JOIN users u ON o.user_id = u.user_id
WHERE u.email LIKE '%@gmail.com'
AND o.status = 'delivered'
AND o.created_at >= '2024-01-01';

-- 5. Use Covering Indexes
-- Create covering index to avoid table lookups
CREATE INDEX idx_orders_covering ON orders(user_id, status, created_at, total_amount);

-- 6. Optimize JOIN Operations
-- Use STRAIGHT_JOIN for specific join order
SELECT STRAIGHT_JOIN
    u.first_name,
    u.last_name,
    o.order_number,
    o.total_amount
FROM users u
JOIN orders o ON u.user_id = o.user_id
WHERE u.created_at >= '2024-01-01'
ORDER BY o.created_at DESC;

-- 7. Use LIMIT with ORDER BY
-- Always use LIMIT with ORDER BY for better performance
SELECT * FROM orders
WHERE status = 'pending'
ORDER BY created_at DESC
LIMIT 20;

-- 8. Optimize Subqueries
-- Replace correlated subqueries with JOINs
-- Slow correlated subquery
SELECT * FROM products p
WHERE p.product_id IN (
    SELECT product_id FROM order_items oi
    WHERE oi.order_id IN (
        SELECT order_id FROM orders o
        WHERE o.status = 'delivered'
    )
);

-- Optimized with JOINs
SELECT DISTINCT p.* FROM products p
JOIN order_items oi ON p.product_id = oi.product_id
JOIN orders o ON oi.order_id = o.order_id
WHERE o.status = 'delivered';

-- 9. Use EXISTS instead of IN for better performance
-- When checking for existence, use EXISTS
SELECT * FROM products p
WHERE EXISTS (
    SELECT 1 FROM order_items oi
    JOIN orders o ON oi.order_id = o.order_id
    WHERE oi.product_id = p.product_id
    AND o.status = 'delivered'
);

-- 10. Optimize GROUP BY queries
-- Use proper indexes for GROUP BY columns
CREATE INDEX idx_orders_user_status ON orders(user_id, status, created_at);

SELECT 
    u.user_id,
    u.first_name,
    u.last_name,
    COUNT(o.order_id) as total_orders,
    SUM(o.total_amount) as total_spent
FROM users u
LEFT JOIN orders o ON u.user_id = o.user_id
WHERE o.status = 'delivered'
GROUP BY u.user_id, u.first_name, u.last_name
HAVING total_orders > 5
ORDER BY total_spent DESC;

-- 11. Use Query Hints for Optimization
-- Force specific index usage
SELECT /*+ USE_INDEX(orders, idx_orders_status_date) */
    order_id,
    order_number,
    total_amount
FROM orders
WHERE status = 'delivered'
AND created_at >= '2024-01-01';

-- 12. Optimize String Operations
-- Use proper string functions and indexes
CREATE INDEX idx_users_email ON users(email);

-- Use prefix matching instead of LIKE with leading wildcard
SELECT * FROM users WHERE email LIKE 'john%'; -- Good
-- Avoid: SELECT * FROM users WHERE email LIKE '%john%'; -- Slow

-- 13. Use Prepared Statements for Repeated Queries
-- Prepare statement for better performance
PREPARE stmt FROM 'SELECT * FROM orders WHERE user_id = ? AND status = ?';
SET @user_id = 1;
SET @status = 'delivered';
EXECUTE stmt USING @user_id, @status;
DEALLOCATE PREPARE stmt;

-- 14. Monitor Query Performance
-- Check query execution time
SET profiling = 1;
SELECT * FROM orders WHERE status = 'delivered';
SHOW PROFILES;
SHOW PROFILE FOR QUERY 1;

-- 15. Use Partitioning for Large Tables
-- Partition orders table by date
ALTER TABLE orders PARTITION BY RANGE (YEAR(created_at)) (
    PARTITION p2023 VALUES LESS THAN (2024),
    PARTITION p2024 VALUES LESS THAN (2025),
    PARTITION p2025 VALUES LESS THAN (2026),
    PARTITION p_future VALUES LESS THAN MAXVALUE
);
```

#### **Q3. How would you implement a comprehensive backup and recovery strategy?**

**Answer:** Here's a complete backup and recovery implementation:

```bash
#!/bin/bash
# MySQL Backup and Recovery Script

# 1. Full Database Backup
mysqldump --single-transaction \
    --routines \
    --triggers \
    --events \
    --all-databases \
    --master-data=2 \
    --flush-logs \
    --hex-blob \
    --default-character-set=utf8mb4 \
    > full_backup_$(date +%Y%m%d_%H%M%S).sql

# 2. Incremental Backup Script
#!/bin/bash
# incremental_backup.sh

BACKUP_DIR="/backup/mysql"
MYSQL_USER="backup_user"
MYSQL_PASSWORD="backup_password"
MYSQL_HOST="localhost"

# Create backup directory
mkdir -p $BACKUP_DIR

# Full backup (run weekly)
if [ "$1" = "full" ]; then
    mysqldump --single-transaction \
        --routines \
        --triggers \
        --events \
        --all-databases \
        --master-data=2 \
        --flush-logs \
        -h $MYSQL_HOST \
        -u $MYSQL_USER \
        -p$MYSQL_PASSWORD \
        > $BACKUP_DIR/full_backup_$(date +%Y%m%d_%H%M%S).sql
fi

# Incremental backup (run daily)
if [ "$1" = "incremental" ]; then
    # Get current binary log position
    CURRENT_LOG=$(mysql -h $MYSQL_HOST -u $MYSQL_USER -p$MYSQL_PASSWORD -e "SHOW MASTER STATUS\G" | grep "File:" | awk '{print $2}')
    CURRENT_POS=$(mysql -h $MYSQL_HOST -u $MYSQL_USER -p$MYSQL_PASSWORD -e "SHOW MASTER STATUS\G" | grep "Position:" | awk '{print $2}')
    
    # Backup binary logs
    mysqlbinlog --start-position=$CURRENT_POS \
        --stop-position=$CURRENT_POS \
        --read-from-remote-server \
        --host=$MYSQL_HOST \
        --user=$MYSQL_USER \
        --password=$MYSQL_PASSWORD \
        $CURRENT_LOG > $BACKUP_DIR/incremental_backup_$(date +%Y%m%d_%H%M%S).sql
fi

# 3. Automated Backup Script with Rotation
#!/bin/bash
# automated_backup.sh

BACKUP_DIR="/backup/mysql"
RETENTION_DAYS=30
MYSQL_USER="backup_user"
MYSQL_PASSWORD="backup_password"
MYSQL_HOST="localhost"

# Create backup directory
mkdir -p $BACKUP_DIR

# Full backup
echo "Starting full backup..."
mysqldump --single-transaction \
    --routines \
    --triggers \
    --events \
    --all-databases \
    --master-data=2 \
    --flush-logs \
    -h $MYSQL_HOST \
    -u $MYSQL_USER \
    -p$MYSQL_PASSWORD \
    | gzip > $BACKUP_DIR/full_backup_$(date +%Y%m%d_%H%M%S).sql.gz

# Compress old backups
find $BACKUP_DIR -name "*.sql" -mtime +1 -exec gzip {} \;

# Remove old backups
find $BACKUP_DIR -name "*.sql.gz" -mtime +$RETENTION_DAYS -delete

echo "Backup completed successfully"

# 4. Point-in-Time Recovery Script
#!/bin/bash
# point_in_time_recovery.sh

BACKUP_DIR="/backup/mysql"
TARGET_TIME="$1"  # Format: 2024-01-15 14:30:00

if [ -z "$TARGET_TIME" ]; then
    echo "Usage: $0 'YYYY-MM-DD HH:MM:SS'"
    exit 1
fi

echo "Starting point-in-time recovery to $TARGET_TIME..."

# Stop MySQL
systemctl stop mysql

# Restore from full backup
LATEST_FULL_BACKUP=$(ls -t $BACKUP_DIR/full_backup_*.sql.gz | head -1)
echo "Restoring from full backup: $LATEST_FULL_BACKUP"
gunzip -c $LATEST_FULL_BACKUP | mysql

# Apply incremental backups up to target time
for backup in $(ls $BACKUP_DIR/incremental_backup_*.sql.gz | sort); do
    BACKUP_TIME=$(stat -c %y $backup | cut -d' ' -f1-2)
    if [[ "$BACKUP_TIME" < "$TARGET_TIME" ]]; then
        echo "Applying incremental backup: $backup"
        gunzip -c $backup | mysql
    fi
done

# Start MySQL
systemctl start mysql

echo "Point-in-time recovery completed"

# 5. Database Recovery Script
#!/bin/bash
# database_recovery.sh

BACKUP_FILE="$1"
DATABASE_NAME="$2"

if [ -z "$BACKUP_FILE" ] || [ -z "$DATABASE_NAME" ]; then
    echo "Usage: $0 <backup_file> <database_name>"
    exit 1
fi

echo "Starting database recovery..."

# Create database if it doesn't exist
mysql -e "CREATE DATABASE IF NOT EXISTS $DATABASE_NAME;"

# Restore database
if [[ "$BACKUP_FILE" == *.gz ]]; then
    gunzip -c $BACKUP_FILE | mysql $DATABASE_NAME
else
    mysql $DATABASE_NAME < $BACKUP_FILE
fi

echo "Database recovery completed successfully"

# 6. Backup Verification Script
#!/bin/bash
# backup_verification.sh

BACKUP_DIR="/backup/mysql"

echo "Verifying backups..."

for backup in $BACKUP_DIR/*.sql.gz; do
    if [ -f "$backup" ]; then
        echo "Verifying: $backup"
        
        # Check if backup file is valid
        if gunzip -t "$backup" 2>/dev/null; then
            echo "âœ“ $backup is valid"
        else
            echo "âœ— $backup is corrupted"
        fi
        
        # Check backup size
        SIZE=$(stat -c %s "$backup")
        if [ $SIZE -gt 1000 ]; then
            echo "âœ“ $backup has reasonable size ($SIZE bytes)"
        else
            echo "âœ— $backup is too small ($SIZE bytes)"
        fi
    fi
done

echo "Backup verification completed"

# 7. MySQL Configuration for Backup
# /etc/mysql/mysql.conf.d/mysqld.cnf

[mysqld]
# Enable binary logging for incremental backups
log-bin = mysql-bin
binlog-format = ROW
expire_logs_days = 7
max_binlog_size = 100M

# Enable general query log for auditing
general_log = 1
general_log_file = /var/log/mysql/mysql.log

# Optimize for backup performance
innodb_flush_log_at_trx_commit = 2
sync_binlog = 0

# 8. Cron Job Setup
# Add to crontab for automated backups
# Full backup every Sunday at 2 AM
0 2 * * 0 /path/to/automated_backup.sh full

# Incremental backup every day at 2 AM
0 2 * * 1-6 /path/to/automated_backup.sh incremental

# Backup verification every day at 3 AM
0 3 * * * /path/to/backup_verification.sh

# 9. Backup Monitoring Script
#!/bin/bash
# backup_monitoring.sh

BACKUP_DIR="/backup/mysql"
ALERT_EMAIL="admin@example.com"

# Check if backups exist
if [ ! -d "$BACKUP_DIR" ] || [ -z "$(ls -A $BACKUP_DIR)" ]; then
    echo "No backups found in $BACKUP_DIR" | mail -s "Backup Alert" $ALERT_EMAIL
    exit 1
fi

# Check if backup is recent (within 24 hours)
LATEST_BACKUP=$(find $BACKUP_DIR -name "*.sql.gz" -mtime -1 | head -1)
if [ -z "$LATEST_BACKUP" ]; then
    echo "No recent backups found" | mail -s "Backup Alert" $ALERT_EMAIL
    exit 1
fi

echo "Backup monitoring completed successfully"

# 10. Restore Specific Tables
#!/bin/bash
# restore_tables.sh

BACKUP_FILE="$1"
DATABASE_NAME="$2"
TABLE_NAMES="$3"

if [ -z "$BACKUP_FILE" ] || [ -z "$DATABASE_NAME" ] || [ -z "$TABLE_NAMES" ]; then
    echo "Usage: $0 <backup_file> <database_name> <table1,table2,...>"
    exit 1
fi

echo "Restoring tables: $TABLE_NAMES"

# Extract specific tables from backup
if [[ "$BACKUP_FILE" == *.gz ]]; then
    gunzip -c $BACKUP_FILE | grep -E "(CREATE TABLE|INSERT INTO) ($TABLE_NAMES)" | mysql $DATABASE_NAME
else
    grep -E "(CREATE TABLE|INSERT INTO) ($TABLE_NAMES)" $BACKUP_FILE | mysql $DATABASE_NAME
fi

echo "Table restoration completed"
```

#### **Q4. How would you implement a high-availability MySQL setup with replication?**

**Answer:** Here's a complete high-availability implementation:

```sql
-- 1. Master Server Configuration
-- /etc/mysql/mysql.conf.d/mysqld.cnf

[mysqld]
# Server identification
server-id = 1
log-bin = mysql-bin
binlog-format = ROW
sync_binlog = 1
innodb_flush_log_at_trx_commit = 1

# Replication settings
expire_logs_days = 7
max_binlog_size = 100M
binlog_cache_size = 1M
max_binlog_cache_size = 2G

# GTID settings
gtid-mode = ON
enforce-gtid-consistency = ON
log-slave-updates = ON

# Performance settings
innodb_buffer_pool_size = 2G
innodb_log_file_size = 256M
innodb_log_files_in_group = 2
innodb_flush_method = O_DIRECT

-- 2. Slave Server Configuration
-- /etc/mysql/mysql.conf.d/mysqld.cnf

[mysqld]
# Server identification
server-id = 2
log-bin = mysql-bin
binlog-format = ROW

# Replication settings
relay-log = mysql-relay-bin
read-only = 1
log-slave-updates = 1

# GTID settings
gtid-mode = ON
enforce-gtid-consistency = ON
log-slave-updates = ON

# Performance settings
innodb_buffer_pool_size = 2G
innodb_log_file_size = 256M
innodb_log_files_in_group = 2

-- 3. Setup Replication User
-- On Master Server
CREATE USER 'repl_user'@'%' IDENTIFIED BY 'secure_replication_password';
GRANT REPLICATION SLAVE ON *.* TO 'repl_user'@'%';
FLUSH PRIVILEGES;

-- 4. Configure Slave Server
-- On Slave Server
CHANGE MASTER TO
    MASTER_HOST = '192.168.1.100',
    MASTER_USER = 'repl_user',
    MASTER_PASSWORD = 'secure_replication_password',
    MASTER_AUTO_POSITION = 1;

-- Start replication
START SLAVE;

-- Check replication status
SHOW SLAVE STATUS\G

-- 5. Multi-Source Replication Setup
-- Configure multiple masters on a single slave
CHANGE MASTER TO
    MASTER_HOST = '192.168.1.100',
    MASTER_USER = 'repl_user',
    MASTER_PASSWORD = 'secure_replication_password',
    MASTER_AUTO_POSITION = 1
FOR CHANNEL 'master1';

CHANGE MASTER TO
    MASTER_HOST = '192.168.1.101',
    MASTER_USER = 'repl_user',
    MASTER_PASSWORD = 'secure_replication_password',
    MASTER_AUTO_POSITION = 1
FOR CHANNEL 'master2';

-- Start both channels
START SLAVE FOR CHANNEL 'master1';
START SLAVE FOR CHANNEL 'master2';

-- 6. Replication Monitoring Script
#!/bin/bash
# replication_monitor.sh

MYSQL_USER="monitor_user"
MYSQL_PASSWORD="monitor_password"
ALERT_EMAIL="admin@example.com"

# Check replication status
REPLICATION_STATUS=$(mysql -u $MYSQL_USER -p$MYSQL_PASSWORD -e "SHOW SLAVE STATUS\G" | grep "Slave_IO_Running\|Slave_SQL_Running\|Seconds_Behind_Master")

IO_RUNNING=$(echo "$REPLICATION_STATUS" | grep "Slave_IO_Running" | awk '{print $2}')
SQL_RUNNING=$(echo "$REPLICATION_STATUS" | grep "Slave_SQL_Running" | awk '{print $2}')
BEHIND_MASTER=$(echo "$REPLICATION_STATUS" | grep "Seconds_Behind_Master" | awk '{print $2}')

# Check if replication is running
if [ "$IO_RUNNING" != "Yes" ] || [ "$SQL_RUNNING" != "Yes" ]; then
    echo "Replication is not running properly" | mail -s "Replication Alert" $ALERT_EMAIL
    exit 1
fi

# Check replication lag
if [ "$BEHIND_MASTER" -gt 60 ]; then
    echo "Replication lag is high: $BEHIND_MASTER seconds" | mail -s "Replication Lag Alert" $ALERT_EMAIL
fi

echo "Replication monitoring completed successfully"

-- 7. Automatic Failover Script
#!/bin/bash
# failover.sh

MASTER_HOST="192.168.1.100"
SLAVE_HOST="192.168.1.101"
MYSQL_USER="admin_user"
MYSQL_PASSWORD="admin_password"

# Check if master is accessible
if ! mysql -h $MASTER_HOST -u $MYSQL_USER -p$MYSQL_PASSWORD -e "SELECT 1" >/dev/null 2>&1; then
    echo "Master server is down, initiating failover..."
    
    # Promote slave to master
    mysql -h $SLAVE_HOST -u $MYSQL_USER -p$MYSQL_PASSWORD -e "STOP SLAVE;"
    mysql -h $SLAVE_HOST -u $MYSQL_USER -p$MYSQL_PASSWORD -e "RESET SLAVE;"
    mysql -h $SLAVE_HOST -u $MYSQL_USER -p$MYSQL_PASSWORD -e "SET GLOBAL read_only = 0;"
    
    echo "Failover completed. Slave promoted to master."
else
    echo "Master server is accessible. No failover needed."
fi

-- 8. Load Balancer Configuration
-- HAProxy configuration for MySQL
global
    daemon
    maxconn 4096

defaults
    mode tcp
    timeout connect 5000ms
    timeout client 50000ms
    timeout server 50000ms

listen mysql-cluster
    bind 0.0.0.0:3306
    mode tcp
    balance roundrobin
    option mysql-check user haproxy_check
    server mysql1 192.168.1.100:3306 check
    server mysql2 192.168.1.101:3306 check backup

-- 9. Backup Strategy for HA Setup
#!/bin/bash
# ha_backup.sh

MASTER_HOST="192.168.1.100"
SLAVE_HOST="192.168.1.101"
BACKUP_DIR="/backup/mysql"

# Backup from slave to avoid master load
mysqldump --single-transaction \
    --routines \
    --triggers \
    --events \
    --all-databases \
    --master-data=2 \
    --flush-logs \
    -h $SLAVE_HOST \
    -u backup_user \
    -pbackup_password \
    | gzip > $BACKUP_DIR/ha_backup_$(date +%Y%m%d_%H%M%S).sql.gz

-- 10. Replication Health Check
#!/bin/bash
# replication_health_check.sh

MYSQL_USER="health_check_user"
MYSQL_PASSWORD="health_check_password"

# Check replication status
mysql -u $MYSQL_USER -p$MYSQL_PASSWORD -e "
SELECT 
    'Replication Status' as Check_Type,
    CASE 
        WHEN Slave_IO_Running = 'Yes' AND Slave_SQL_Running = 'Yes' 
        THEN 'OK' 
        ELSE 'FAIL' 
    END as Status,
    Seconds_Behind_Master as Lag_Seconds
FROM information_schema.SLAVE_STATUS;
"

# Check for replication errors
mysql -u $MYSQL_USER -p$MYSQL_PASSWORD -e "
SELECT 
    'Error Check' as Check_Type,
    CASE 
        WHEN Last_IO_Error = '' AND Last_SQL_Error = '' 
        THEN 'OK' 
        ELSE 'FAIL' 
    END as Status,
    CONCAT(Last_IO_Error, ' ', Last_SQL_Error) as Error_Details
FROM information_schema.SLAVE_STATUS;
"
```

#### **Q5. How would you implement a comprehensive monitoring and alerting system?**

**Answer:** Here's a complete monitoring implementation:

```sql
-- 1. Performance Schema Monitoring
-- Monitor slow queries
SELECT 
    DIGEST_TEXT,
    COUNT_STAR,
    AVG_TIMER_WAIT/1000000000 as AVG_TIME_SEC,
    MAX_TIMER_WAIT/1000000000 as MAX_TIME_SEC,
    SUM_ROWS_EXAMINED,
    SUM_ROWS_SENT
FROM performance_schema.events_statements_summary_by_digest
ORDER BY AVG_TIMER_WAIT DESC
LIMIT 10;

-- Monitor table I/O
SELECT 
    OBJECT_SCHEMA,
    OBJECT_NAME,
    COUNT_READ,
    COUNT_WRITE,
    SUM_TIMER_READ/1000000000 as READ_TIME_SEC,
    SUM_TIMER_WRITE/1000000000 as WRITE_TIME_SEC
FROM performance_schema.table_io_waits_summary_by_table
ORDER BY SUM_TIMER_READ + SUM_TIMER_WRITE DESC
LIMIT 10;

-- Monitor connection usage
SELECT 
    USER,
    HOST,
    COUNT(*) as CONNECTIONS,
    SUM(CASE WHEN COMMAND != 'Sleep' THEN 1 ELSE 0 END) as ACTIVE_CONNECTIONS
FROM information_schema.PROCESSLIST
GROUP BY USER, HOST
ORDER BY CONNECTIONS DESC;

-- 2. Custom Monitoring Tables
CREATE TABLE monitoring_metrics (
    metric_id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    metric_name VARCHAR(100) NOT NULL,
    metric_value DECIMAL(15,4) NOT NULL,
    metric_unit VARCHAR(20),
    recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_metric_name (metric_name),
    INDEX idx_recorded_at (recorded_at)
) ENGINE=InnoDB;

-- 3. Monitoring Procedures
DELIMITER //

CREATE PROCEDURE CollectSystemMetrics()
BEGIN
    DECLARE done INT DEFAULT FALSE;
    DECLARE metric_name VARCHAR(100);
    DECLARE metric_value DECIMAL(15,4);
    
    -- Collect connection metrics
    INSERT INTO monitoring_metrics (metric_name, metric_value, metric_unit)
    SELECT 'connections_total', COUNT(*), 'count'
    FROM information_schema.PROCESSLIST;
    
    INSERT INTO monitoring_metrics (metric_name, metric_value, metric_unit)
    SELECT 'connections_active', COUNT(*), 'count'
    FROM information_schema.PROCESSLIST
    WHERE COMMAND != 'Sleep';
    
    -- Collect query metrics
    INSERT INTO monitoring_metrics (metric_name, metric_value, metric_unit)
    SELECT 'queries_per_second', 
           (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Queries'),
           'queries/sec';
    
    -- Collect buffer pool metrics
    INSERT INTO monitoring_metrics (metric_name, metric_value, metric_unit)
    SELECT 'innodb_buffer_pool_hit_ratio',
           (1 - (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Innodb_buffer_pool_reads') /
                 (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Innodb_buffer_pool_read_requests')) * 100,
           'percent';
    
    -- Collect table size metrics
    INSERT INTO monitoring_metrics (metric_name, metric_value, metric_unit)
    SELECT 'database_size_mb',
           SUM(DATA_LENGTH + INDEX_LENGTH) / 1024 / 1024,
           'MB'
    FROM information_schema.TABLES
    WHERE TABLE_SCHEMA NOT IN ('information_schema', 'performance_schema', 'mysql', 'sys');
    
END //

DELIMITER ;

-- 4. Automated Monitoring Script
#!/bin/bash
# mysql_monitoring.sh

MYSQL_USER="monitor_user"
MYSQL_PASSWORD="monitor_password"
MYSQL_HOST="localhost"
ALERT_EMAIL="admin@example.com"

# Function to send alert
send_alert() {
    local subject="$1"
    local message="$2"
    echo "$message" | mail -s "$subject" $ALERT_EMAIL
}

# Check MySQL service status
if ! systemctl is-active --quiet mysql; then
    send_alert "MySQL Service Down" "MySQL service is not running"
    exit 1
fi

# Check connection count
CONNECTIONS=$(mysql -h $MYSQL_HOST -u $MYSQL_USER -p$MYSQL_PASSWORD -e "SELECT COUNT(*) FROM information_schema.PROCESSLIST" -s -N)
if [ $CONNECTIONS -gt 100 ]; then
    send_alert "High Connection Count" "MySQL has $CONNECTIONS active connections"
fi

# Check replication lag
if mysql -h $MYSQL_HOST -u $MYSQL_USER -p$MYSQL_PASSWORD -e "SHOW SLAVE STATUS\G" | grep -q "Slave_IO_Running: Yes"; then
    LAG=$(mysql -h $MYSQL_HOST -u $MYSQL_USER -p$MYSQL_PASSWORD -e "SHOW SLAVE STATUS\G" | grep "Seconds_Behind_Master" | awk '{print $2}')
    if [ "$LAG" != "NULL" ] && [ $LAG -gt 60 ]; then
        send_alert "High Replication Lag" "Replication lag is $LAG seconds"
    fi
fi

# Check disk space
DISK_USAGE=$(df /var/lib/mysql | tail -1 | awk '{print $5}' | sed 's/%//')
if [ $DISK_USAGE -gt 80 ]; then
    send_alert "High Disk Usage" "MySQL disk usage is $DISK_USAGE%"
fi

# Check slow queries
SLOW_QUERIES=$(mysql -h $MYSQL_HOST -u $MYSQL_USER -p$MYSQL_PASSWORD -e "SHOW GLOBAL STATUS LIKE 'Slow_queries'" -s -N | awk '{print $2}')
if [ $SLOW_QUERIES -gt 100 ]; then
    send_alert "High Slow Query Count" "MySQL has $SLOW_QUERIES slow queries"
fi

echo "Monitoring check completed successfully"

-- 5. Performance Monitoring Dashboard
CREATE VIEW performance_dashboard AS
SELECT 
    'Connections' as Metric,
    COUNT(*) as Current_Value,
    'count' as Unit,
    NOW() as Timestamp
FROM information_schema.PROCESSLIST
UNION ALL
SELECT 
    'Active Connections',
    COUNT(*),
    'count',
    NOW()
FROM information_schema.PROCESSLIST
WHERE COMMAND != 'Sleep'
UNION ALL
SELECT 
    'Buffer Pool Hit Ratio',
    ROUND((1 - (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Innodb_buffer_pool_reads') /
                 (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Innodb_buffer_pool_read_requests')) * 100, 2),
    'percent',
    NOW()
UNION ALL
SELECT 
    'Query Cache Hit Ratio',
    ROUND((SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Qcache_hits') /
          (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Qcache_hits') +
          (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Qcache_inserts') * 100, 2),
    'percent',
    NOW();

-- 6. Alerting Rules
CREATE TABLE alert_rules (
    rule_id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    rule_name VARCHAR(100) NOT NULL,
    metric_name VARCHAR(100) NOT NULL,
    threshold_value DECIMAL(15,4) NOT NULL,
    comparison_operator ENUM('>', '<', '>=', '<=', '=', '!=') NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_metric_name (metric_name),
    INDEX idx_is_active (is_active)
) ENGINE=InnoDB;

-- Insert default alert rules
INSERT INTO alert_rules (rule_name, metric_name, threshold_value, comparison_operator) VALUES
('High Connections', 'connections_total', 100, '>'),
('High Replication Lag', 'replication_lag_seconds', 60, '>'),
('Low Buffer Pool Hit Ratio', 'innodb_buffer_pool_hit_ratio', 95, '<'),
('High Disk Usage', 'disk_usage_percent', 80, '>');

-- 7. Monitoring Data Retention
CREATE EVENT cleanup_old_metrics
ON SCHEDULE EVERY 1 DAY
STARTS CURRENT_TIMESTAMP
DO
DELETE FROM monitoring_metrics 
WHERE recorded_at < DATE_SUB(NOW(), INTERVAL 30 DAY);

-- 8. Real-time Monitoring Script
#!/bin/bash
# real_time_monitoring.sh

MYSQL_USER="monitor_user"
MYSQL_PASSWORD="monitor_password"
MYSQL_HOST="localhost"

while true; do
    # Collect metrics
    CONNECTIONS=$(mysql -h $MYSQL_HOST -u $MYSQL_USER -p$MYSQL_PASSWORD -e "SELECT COUNT(*) FROM information_schema.PROCESSLIST" -s -N)
    ACTIVE_CONNECTIONS=$(mysql -h $MYSQL_HOST -u $MYSQL_USER -p$MYSQL_PASSWORD -e "SELECT COUNT(*) FROM information_schema.PROCESSLIST WHERE COMMAND != 'Sleep'" -s -N)
    
    # Store metrics
    mysql -h $MYSQL_HOST -u $MYSQL_USER -p$MYSQL_PASSWORD -e "
    INSERT INTO monitoring_metrics (metric_name, metric_value, metric_unit) VALUES
    ('connections_total', $CONNECTIONS, 'count'),
    ('connections_active', $ACTIVE_CONNECTIONS, 'count');
    "
    
    # Wait 30 seconds
    sleep 30
done

-- 9. Health Check Endpoint
CREATE PROCEDURE HealthCheck()
BEGIN
    DECLARE health_status VARCHAR(20) DEFAULT 'OK';
    DECLARE error_message TEXT DEFAULT '';
    
    -- Check if MySQL is responding
    IF NOT EXISTS (SELECT 1 FROM information_schema.PROCESSLIST LIMIT 1) THEN
        SET health_status = 'ERROR';
        SET error_message = 'MySQL is not responding';
    END IF;
    
    -- Check replication status
    IF EXISTS (SELECT 1 FROM information_schema.SLAVE_STATUS) THEN
        IF NOT EXISTS (SELECT 1 FROM information_schema.SLAVE_STATUS 
                      WHERE Slave_IO_Running = 'Yes' AND Slave_SQL_Running = 'Yes') THEN
            SET health_status = 'WARNING';
            SET error_message = CONCAT(error_message, '; Replication is not running properly');
        END IF;
    END IF;
    
    -- Check disk space
    IF EXISTS (SELECT 1 FROM information_schema.TABLES 
              WHERE TABLE_SCHEMA = 'information_schema' 
              AND DATA_LENGTH > 1000000000) THEN
        SET health_status = 'WARNING';
        SET error_message = CONCAT(error_message, '; High disk usage detected');
    END IF;
    
    SELECT health_status as status, error_message as message, NOW() as timestamp;
END;

-- 10. Monitoring Dashboard Query
SELECT 
    DATE(recorded_at) as date,
    metric_name,
    AVG(metric_value) as avg_value,
    MAX(metric_value) as max_value,
    MIN(metric_value) as min_value
FROM monitoring_metrics
WHERE recorded_at >= DATE_SUB(NOW(), INTERVAL 7 DAY)
GROUP BY DATE(recorded_at), metric_name
ORDER BY date DESC, metric_name;
```

#### **Q6. How would you implement a comprehensive security hardening strategy?**

**Answer:** Here's a complete security implementation:

```sql
-- 1. User Management and Privileges
-- Create application-specific users with minimal privileges
CREATE USER 'app_user'@'localhost' IDENTIFIED BY 'strong_password_123!';
CREATE USER 'readonly_user'@'192.168.1.%' IDENTIFIED BY 'readonly_pass_456!';
CREATE USER 'backup_user'@'localhost' IDENTIFIED BY 'backup_pass_789!';

-- Grant minimal required privileges
GRANT SELECT, INSERT, UPDATE, DELETE ON ecommerce.* TO 'app_user'@'localhost';
GRANT SELECT ON ecommerce.products TO 'readonly_user'@'192.168.1.%';
GRANT SELECT, LOCK TABLES, RELOAD, REPLICATION CLIENT ON *.* TO 'backup_user'@'localhost';

-- Remove anonymous users
DELETE FROM mysql.user WHERE User = '';
DELETE FROM mysql.user WHERE User = 'root' AND Host NOT IN ('localhost', '127.0.0.1', '::1');

-- Remove test database
DROP DATABASE IF EXISTS test;
DELETE FROM mysql.db WHERE Db = 'test' OR Db = 'test\\_%';

-- 2. Password Policy Implementation
-- Install password validation plugin
INSTALL PLUGIN validate_password SONAME 'validate_password.so';

-- Configure password policy
SET GLOBAL validate_password.policy = STRONG;
SET GLOBAL validate_password.length = 12;
SET GLOBAL validate_password.mixed_case_count = 2;
SET GLOBAL validate_password.number_count = 2;
SET GLOBAL validate_password.special_char_count = 2;

-- 3. SSL/TLS Configuration
-- Generate SSL certificates
-- mysql_ssl_rsa_setup

-- Configure SSL in my.cnf
-- [mysqld]
-- ssl-ca = /etc/mysql/ssl/ca.pem
-- ssl-cert = /etc/mysql/ssl/server-cert.pem
-- ssl-key = /etc/mysql/ssl/server-key.pem

-- Require SSL for specific users
ALTER USER 'app_user'@'localhost' REQUIRE SSL;
ALTER USER 'readonly_user'@'192.168.1.%' REQUIRE SSL;

-- 4. Audit Logging
-- Install audit log plugin
INSTALL PLUGIN audit_log SONAME 'audit_log.so';

-- Configure audit log
SET GLOBAL audit_log_policy = ALL;
SET GLOBAL audit_log_format = JSON;
SET GLOBAL audit_log_file = '/var/log/mysql/audit.log';

-- 5. Data Encryption
-- Enable encryption at rest
-- Configure in my.cnf
-- [mysqld]
-- innodb_encrypt_tables = ON
-- innodb_encryption_threads = 4
-- innodb_encryption_rotate_key_age = 1

-- Encrypt specific tables
ALTER TABLE users ENCRYPTION = 'Y';
ALTER TABLE orders ENCRYPTION = 'Y';
ALTER TABLE payments ENCRYPTION = 'Y';

-- Encrypt specific columns
CREATE TABLE sensitive_data (
    id INT PRIMARY KEY,
    name VARCHAR(100),
    ssn VARBINARY(255), -- Encrypted SSN
    credit_card VARBINARY(255) -- Encrypted credit card
);

-- 6. Access Control and Firewall
-- Create firewall rules
-- iptables -A INPUT -p tcp --dport 3306 -s 192.168.1.0/24 -j ACCEPT
-- iptables -A INPUT -p tcp --dport 3306 -j DROP

-- Configure MySQL to bind to specific interface
-- bind-address = 127.0.0.1

-- 7. Security Monitoring
-- Monitor failed login attempts
CREATE TABLE security_events (
    event_id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    event_type VARCHAR(50) NOT NULL,
    user_name VARCHAR(100),
    host VARCHAR(100),
    event_description TEXT,
    ip_address VARCHAR(45),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_event_type (event_type),
    INDEX idx_created_at (created_at)
) ENGINE=InnoDB;

-- Create trigger to log failed logins
DELIMITER //
CREATE TRIGGER log_failed_login
AFTER INSERT ON mysql.general_log
FOR EACH ROW
BEGIN
    IF NEW.argument LIKE '%Access denied%' THEN
        INSERT INTO security_events (event_type, user_name, host, event_description, ip_address)
        VALUES ('failed_login', NEW.user_host, NEW.host, NEW.argument, NEW.host);
    END IF;
END //
DELIMITER ;

-- 8. Data Masking and Anonymization
-- Create function to mask sensitive data
DELIMITER //
CREATE FUNCTION mask_email(email VARCHAR(255))
RETURNS VARCHAR(255)
READS SQL DATA
DETERMINISTIC
BEGIN
    DECLARE masked_email VARCHAR(255);
    DECLARE at_pos INT;
    DECLARE domain_part VARCHAR(255);
    
    SET at_pos = LOCATE('@', email);
    IF at_pos > 0 THEN
        SET domain_part = SUBSTRING(email, at_pos);
        SET masked_email = CONCAT(LEFT(email, 2), '***', domain_part);
    ELSE
        SET masked_email = '***';
    END IF;
    
    RETURN masked_email;
END //
DELIMITER ;

-- Create function to mask phone numbers
DELIMITER //
CREATE FUNCTION mask_phone(phone VARCHAR(20))
RETURNS VARCHAR(20)
READS SQL DATA
DETERMINISTIC
BEGIN
    DECLARE masked_phone VARCHAR(20);
    
    IF LENGTH(phone) >= 4 THEN
        SET masked_phone = CONCAT(LEFT(phone, 2), '***', RIGHT(phone, 2));
    ELSE
        SET masked_phone = '***';
    END IF;
    
    RETURN masked_phone;
END //
DELIMITER ;

-- 9. Security Compliance Checks
-- Create procedure to check security compliance
DELIMITER //
CREATE PROCEDURE CheckSecurityCompliance()
BEGIN
    DECLARE compliance_score INT DEFAULT 0;
    DECLARE total_checks INT DEFAULT 0;
    DECLARE check_result VARCHAR(100);
    
    -- Check 1: Anonymous users
    SET total_checks = total_checks + 1;
    IF NOT EXISTS (SELECT 1 FROM mysql.user WHERE User = '') THEN
        SET compliance_score = compliance_score + 1;
        SET check_result = 'PASS: No anonymous users';
    ELSE
        SET check_result = 'FAIL: Anonymous users found';
    END IF;
    SELECT check_result as result;
    
    -- Check 2: Root remote access
    SET total_checks = total_checks + 1;
    IF NOT EXISTS (SELECT 1 FROM mysql.user WHERE User = 'root' AND Host != 'localhost') THEN
        SET compliance_score = compliance_score + 1;
        SET check_result = 'PASS: Root remote access disabled';
    ELSE
        SET check_result = 'FAIL: Root remote access enabled';
    END IF;
    SELECT check_result as result;
    
    -- Check 3: Test database
    SET total_checks = total_checks + 1;
    IF NOT EXISTS (SELECT 1 FROM information_schema.SCHEMATA WHERE SCHEMA_NAME = 'test') THEN
        SET compliance_score = compliance_score + 1;
        SET check_result = 'PASS: Test database removed';
    ELSE
        SET check_result = 'FAIL: Test database exists';
    END IF;
    SELECT check_result as result;
    
    -- Check 4: SSL configuration
    SET total_checks = total_checks + 1;
    IF EXISTS (SELECT 1 FROM information_schema.GLOBAL_VARIABLES WHERE VARIABLE_NAME = 'ssl_ca' AND VARIABLE_VALUE != '') THEN
        SET compliance_score = compliance_score + 1;
        SET check_result = 'PASS: SSL configured';
    ELSE
        SET check_result = 'FAIL: SSL not configured';
    END IF;
    SELECT check_result as result;
    
    -- Final score
    SELECT 
        compliance_score as passed_checks,
        total_checks as total_checks,
        ROUND((compliance_score / total_checks) * 100, 2) as compliance_percentage;
END //
DELIMITER ;

-- 10. Automated Security Monitoring Script
#!/bin/bash
# security_monitoring.sh

MYSQL_USER="security_monitor"
MYSQL_PASSWORD="security_monitor_pass"
ALERT_EMAIL="security@example.com"

# Function to send security alert
send_security_alert() {
    local subject="$1"
    local message="$2"
    echo "$message" | mail -s "SECURITY ALERT: $subject" $ALERT_EMAIL
}

# Check for failed login attempts
FAILED_LOGINS=$(mysql -u $MYSQL_USER -p$MYSQL_PASSWORD -e "
SELECT COUNT(*) FROM security_events 
WHERE event_type = 'failed_login' 
AND created_at >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
" -s -N)

if [ $FAILED_LOGINS -gt 10 ]; then
    send_security_alert "High Failed Login Attempts" "There have been $FAILED_LOGINS failed login attempts in the last hour"
fi

# Check for suspicious queries
SUSPICIOUS_QUERIES=$(mysql -u $MYSQL_USER -p$MYSQL_PASSWORD -e "
SELECT COUNT(*) FROM mysql.general_log 
WHERE argument LIKE '%DROP%' 
OR argument LIKE '%DELETE%' 
OR argument LIKE '%TRUNCATE%'
AND event_time >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
" -s -N)

if [ $SUSPICIOUS_QUERIES -gt 5 ]; then
    send_security_alert "Suspicious Queries Detected" "There have been $SUSPICIOUS_QUERIES suspicious queries in the last hour"
fi

# Check for privilege escalation attempts
PRIVILEGE_ATTEMPTS=$(mysql -u $MYSQL_USER -p$MYSQL_PASSWORD -e "
SELECT COUNT(*) FROM mysql.general_log 
WHERE argument LIKE '%GRANT%' 
OR argument LIKE '%REVOKE%'
AND event_time >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
" -s -N)

if [ $PRIVILEGE_ATTEMPTS -gt 3 ]; then
    send_security_alert "Privilege Escalation Attempts" "There have been $PRIVILEGE_ATTEMPTS privilege escalation attempts in the last hour"
fi

echo "Security monitoring completed successfully"

-- 11. Data Backup Security
-- Encrypt backup files
mysqldump --single-transaction --routines --triggers --all-databases \
    | gzip | openssl enc -aes-256-cbc -salt -out backup_$(date +%Y%m%d_%H%M%S).sql.gz.enc \
    -pass pass:backup_encryption_key

-- 12. Regular Security Updates
#!/bin/bash
# security_updates.sh

# Update MySQL to latest version
apt update
apt upgrade mysql-server -y

# Restart MySQL service
systemctl restart mysql

# Run security compliance check
mysql -u $MYSQL_USER -p$MYSQL_PASSWORD -e "CALL CheckSecurityCompliance();"

echo "Security updates completed successfully"
```

#### **Q7. How would you implement a comprehensive performance tuning strategy?**

**Answer:** Here's a complete performance tuning implementation:

```sql
-- 1. Query Performance Analysis
-- Analyze slow queries
SELECT 
    query_time,
    lock_time,
    rows_sent,
    rows_examined,
    sql_text
FROM mysql.slow_log
WHERE start_time > DATE_SUB(NOW(), INTERVAL 1 HOUR)
ORDER BY query_time DESC
LIMIT 10;

-- Analyze query execution plans
EXPLAIN FORMAT=JSON
SELECT 
    u.first_name,
    u.last_name,
    o.order_number,
    o.total_amount,
    p.product_name,
    oi.quantity
FROM users u
JOIN orders o ON u.user_id = o.user_id followed by the rest of the implementation

-- 2. Index Optimization
-- Analyze index usage
SELECT 
    TABLE_SCHEMA,
    TABLE_NAME,
    INDEX_NAME,
    CARDINALITY,
    SEQ_IN_INDEX,
    COLUMN_NAME
FROM information_schema.STATISTICS
WHERE TABLE_SCHEMA = 'ecommerce'
ORDER BY TABLE_NAME, INDEX_NAME, SEQ_IN_INDEX;

-- Find unused indexes
SELECT 
    s.TABLE_SCHEMA,
    s.TABLE_NAME,
    s.INDEX_NAME
FROM information_schema.STATISTICS s
LEFT JOIN performance_schema.table_io_waits_summary_by_index_usage i
    ON s.TABLE_SCHEMA = i.OBJECT_SCHEMA
    AND s.TABLE_NAME = i.OBJECT_NAME
    AND s.INDEX_NAME = i.INDEX_NAME
WHERE s.TABLE_SCHEMA NOT IN ('mysql', 'performance_schema', 'information_schema')
AND i.INDEX_NAME IS NULL
AND s.INDEX_NAME != 'PRIMARY';

-- Create optimized indexes
CREATE INDEX idx_orders_user_status_date ON orders(user_id, status, created_at);
CREATE INDEX idx_products_category_active ON products(category_id, is_active, price);
CREATE INDEX idx_order_items_product_order ON order_items(product_id, order_id);

-- 3. Buffer Pool Optimization
-- Monitor buffer pool usage
SELECT 
    POOL_ID,
    POOL_SIZE,
    FREE_BUFFERS,
    DATABASE_PAGES,
    OLD_DATABASE_PAGES,
    MODIFIED_DATABASE_PAGES
FROM performance_schema.innodb_buffer_pool_stats;

-- Optimize buffer pool size
-- Set in my.cnf: innodb_buffer_pool_size = 2G (70-80% of RAM)

-- 4. Connection Optimization
-- Monitor connection usage
SELECT 
    USER,
    HOST,
    COUNT(*) as CONNECTIONS,
    SUM(CASE WHEN COMMAND != 'Sleep' THEN 1 ELSE 0 END) as ACTIVE_CONNECTIONS
FROM information_schema.PROCESSLIST
GROUP BY USER, HOST
ORDER BY CONNECTIONS DESC;

-- Optimize connection settings
-- Set in my.cnf:
-- max_connections = 200
-- max_connect_errors = 100000
-- thread_cache_size = 8

-- 5. Query Cache Optimization
-- Monitor query cache
SELECT 
    VARIABLE_NAME,
    VARIABLE_VALUE
FROM information_schema.GLOBAL_STATUS
WHERE VARIABLE_NAME LIKE 'Qcache%';

-- Optimize query cache
-- Set in my.cnf:
-- query_cache_type = 1
-- query_cache_size = 64M
-- query_cache_limit = 2M

-- 6. Temporary Table Optimization
-- Monitor temporary table usage
SELECT 
    VARIABLE_NAME,
    VARIABLE_VALUE
FROM information_schema.GLOBAL_STATUS
WHERE VARIABLE_NAME LIKE 'Created_tmp%';

-- Optimize temporary table settings
-- Set in my.cnf:
-- tmp_table_size = 64M
-- max_heap_table_size = 64M

-- 7. Sort Buffer Optimization
-- Monitor sort operations
SELECT 
    VARIABLE_NAME,
    VARIABLE_VALUE
FROM information_schema.GLOBAL_STATUS
WHERE VARIABLE_NAME LIKE 'Sort%';

-- Optimize sort buffer
-- Set in my.cnf:
-- sort_buffer_size = 2M
-- read_buffer_size = 2M
-- read_rnd_buffer_size = 8M

-- 8. InnoDB Optimization
-- Monitor InnoDB status
SHOW ENGINE INNODB STATUS;

-- Optimize InnoDB settings
-- Set in my.cnf:
-- innodb_buffer_pool_size = 2G
-- innodb_log_file_size = 256M
-- innodb_log_files_in_group = 2
-- innodb_flush_method = O_DIRECT
-- innodb_thread_concurrency = 0
-- innodb_io_capacity = 2000
-- innodb_io_capacity_max = 4000

-- 9. Performance Monitoring
-- Create performance monitoring view
CREATE VIEW performance_monitoring AS
SELECT 
    'Connections' as Metric,
    COUNT(*) as Current_Value,
    'count' as Unit
FROM information_schema.PROCESSLIST
UNION ALL
SELECT 
    'Active Connections',
    COUNT(*),
    'count'
FROM information_schema.PROCESSLIST
WHERE COMMAND != 'Sleep'
UNION ALL
SELECT 
    'Buffer Pool Hit Ratio',
    ROUND((1 - (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Innodb_buffer_pool_reads') /
                 (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Innodb_buffer_pool_read_requests')) * 100, 2),
    'percent'
UNION ALL
SELECT 
    'Query Cache Hit Ratio',
    ROUND((SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Qcache_hits') /
          ((SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Qcache_hits') +
           (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Qcache_inserts')) * 100, 2),
    'percent';

-- 10. Automated Performance Tuning Script
#!/bin/bash
# performance_tuning.sh

MYSQL_USER="performance_monitor"
MYSQL_PASSWORD="performance_monitor_pass"

# Check buffer pool hit ratio
BUFFER_POOL_HIT_RATIO=$(mysql -u $MYSQL_USER -p$MYSQL_PASSWORD -e "
SELECT ROUND((1 - (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Innodb_buffer_pool_reads') /
                 (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Innodb_buffer_pool_read_requests')) * 100, 2)
" -s -N)

if [ $BUFFER_POOL_HIT_RATIO -lt 95 ]; then
    echo "Warning: Buffer pool hit ratio is $BUFFER_POOL_HIT_RATIO% (should be > 95%)"
    echo "Consider increasing innodb_buffer_pool_size"
fi

# Check query cache hit ratio
QUERY_CACHE_HIT_RATIO=$(mysql -u $MYSQL_USER -p$MYSQL_PASSWORD -e "
SELECT ROUND((SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Qcache_hits') /
          ((SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Qcache_hits') +
           (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME = 'Qcache_inserts')) * 100, 2)
" -s -N)

if [ $QUERY_CACHE_HIT_RATIO -lt 80 ]; then
    echo "Warning: Query cache hit ratio is $QUERY_CACHE_HIT_RATIO% (should be > 80%)"
    echo "Consider increasing query_cache_size"
fi

# Check connection usage
CONNECTIONS=$(mysql -u $MYSQL_USER -p$MYSQL_PASSWORD -e "SELECT COUNT(*) FROM information_schema.PROCESSLIST" -s -N)
MAX_CONNECTIONS=$(mysql -u $MYSQL_USER -p$MYSQL_PASSWORD -e "SHOW VARIABLES LIKE 'max_connections'" -s -N | awk '{print $2}')

CONNECTION_USAGE=$((CONNECTIONS * 100 / MAX_CONNECTIONS))
if [ $CONNECTION_USAGE -gt 80 ]; then
    echo "Warning: Connection usage is $CONNECTION_USAGE% (should be < 80%)"
    echo "Consider increasing max_connections"
fi

echo "Performance tuning check completed successfully"
```

---

## ðŸŽ¯ SUMMARY

**Total MySQL Practical Questions: 7+ with complete implementations covering:**

âœ… **E-commerce Database Design**  
âœ… **Query Optimization**  
âœ… **Backup & Recovery**  
âœ… **High Availability & Replication**  
âœ… **Monitoring & Alerting**  
âœ… **Security Hardening**  
âœ… **Performance Tuning**

Each question includes:
- **Complete working code**
- **Real-world examples**
- **Best practices**
- **Error handling**
- **Performance optimization**
- **Security considerations**

**Perfect for senior MySQL developer interviews! ðŸš€**

